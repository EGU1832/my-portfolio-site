# 시스템 프로그래밍  

<br>

[[CSAPP] 포스팅을 시작하며.. (tistory.com)](https://it-eldorado.tistory.com/32)  

<br>

본 교과목은 **컴퓨터 시스템 소프트웨어의 기본적인 개념** 및 **구체적인 설계**와 **구현 방법**을 프로젝트 진행 위주로 학습한다. 이를 통해 컴퓨터 내부에서 자료들이 처리되는 기본 원리를 이해 하고 시스템 소프트웨어를 설계하며 **고급 리눅스 프로그래밍**을 위한 이론 및 실습을 한다.  
단일 컴퓨터 내에서의 주요 시스템 소프트웨어로서 **링킹, 프로세스, 예외처리, 시그널, 시스템 수준 I/O, Concurrent(동시성) 프로그래밍과 동기화 기법, 동적 메모리 할당, 가상메모리 개념**을 학습하고 이 와 연관된 리눅스 프로그래밍을 통하여 시스템과 연관된 프로그래밍에 관한 이론을 배우고 구현 을 해보도록 한다.  

<br>

프로젝트와 병행되는만큼 선수과목을 잘 수강하여야 한다. 요구되는 기초 지식은 다음과 같다.  
1. C 언어  
2. 자료구조  
3. 컴퓨터 아키텍쳐, 어셈블리 언어 (선택)  

<br>

CPU 입장에서 프로그램을 구동시킨다는 게 뭘까? 명령어들을 그냥 순차적으로 실행만 하는 것  

<br>

Reference  
[동시성 프로그래밍 Concurrent Programming (velog.io)](https://velog.io/@jungbumwoo/%EB%8F%99%EC%8B%9C%EC%84%B1-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-Concurrent-Programming)  

<br>

프로젝트  
1. 첫 번쨰 프로젝트  
- shell utility 구현...?  
- IPC(inter program comunication)  
2. 두 번째 프로젝트  
- 주식서버 만들기  
- 서버에 대한 대응과 클라이언트 구현  
3. 세 번쨰 프로젝트  
- comming soon..  

<br>

# Computer System Architecture 101  

<br>

컴퓨터 안에는 CPU와 memory가 있다. Assembly가 기계 언어로 우리가 짠 코드들을 바꿔주고, 그것을 CPU가 이해하여 명령들을 실행한다.  
CPU는 명령들을 sequencial하게 실행하게 되어있다.  
이렇게 명령 순차적으로 실행하게 하기 위해서 CPU는 **그 다음에 읽을 명령어의 주솟값**을 가지고 있다.  

<br>

**Address bus**: Data Bus의 데이터가 어디서 또는 어디로 이동해야하는지 결정한다. CPU/Memory 사이의 주소값을 전달한다.  
**Data bus**: 시스템 모듈 사이에 테이터가 이동하기 위한 길을 제공해준다.  
**Word**: 읽고 쓰기를 할 수 있는 가장 작은 단위  

<br>

주소가 표시되어있다. 64bit 컴퓨터는 +8 씩 커진다. 옛날 8bit 컴퓨터라 가정했으니 그림에서는 주소가 +1 씩 커진다.  
Memory에 들어가있는 contents(program)는 CPU 설계자들이 정한다. 이 명령어 set는 CPU마다 조금씩 조금씩 다르다.<img src="Docs/Pasted image 20240307151524.png">
  

<br>

CPU 안의 PC(program counter)라는 register에 프로그램이 실행될 때의 시작 주소가 들어가있다.  
메모리에 특정 변수를 어디에 할당할지는 우리가 결정할 필요 없이 OS가 해준다.  
CPU입장에서는 Assembly 언어로 적혀진 명령들을 순차적으로 실행시키기만 하면 되는 것이다.  

<br>

실행하기 위한 단계는 크게 기본적으로 다음과 같다.  
> IF(instruction fetch), ID(instruction decode), Exacute instruction  

<br>

register(IR)에서 node를 가지고 오면 그게 뭔지 CPU가 이해하기 위해서 앞의 8개의 비트는 OP 코드로 보겠다는 약속을 미리 한다.  
CPU는 그것을 통해 실행할 명령어를 알아내고, 그 다음 비트를 순차적으로 읽으면서 다뤄야할 데이터, 그게 들어있는 메모리 등을 이해하고 실행한다.  
이제 Program counter는 clock의 싱크에 맞게 주솟값을 계속 count한다.  
[Pipline - Front Study (hangem-study.readthedocs.io)](https://hangem-study.readthedocs.io/en/latest/etc/pipline/)  
[26. Pipelined datapath : 네이버 블로그 (naver.com)](https://blog.naver.com/mayooyos/220876757681)  

<br>

# 1. Exceptional Control Flow: Exceptions and Process  

<br>

## 1.1 Control Flow  

<br>

- 현재 CPU는 병렬처리를 하지만, 지금은 개념을 좀 더 쉽게 하기 위해 한 번에 하나씩 실행한다고 이해하자.  
- 명령을 이해할 수 있도록 복호화(decoding, interpret)하고, 실행한다.  
- main() 처음에서 끝까지, 쭉 실행한다고 가정을 하자.  
- 이를 우리는 CPU 입장에서의 제어 흐름으로써, 물리적인 흐름이라고 한다.  

<br>

## 1.2 Altering the Control Flow  

<br>

- Program state에서 control flow 바꾸기  
1. Jumps and branches (low-level에서 발생)  
```c
for{}
while{}
```
이렇게 하면 flow가 loop를 돌도록 바뀐다.  
2. Call and return (Subroutine 함수 호출 시)  
```c
static linking
void my_function() {...}
```
이렇게 코드를 짜면 flow가 lib에서 데이터를 읽고 다시 inst으로 돌아가도록 바뀐다.  
- CPU 입장에서는 다시 돌아가는 것, 즉 **Jump cost**가 커지므로 좋지 않다. 다른 데로 점프를 해도 돌아올 때 어디로 와야 한다는 걸 기록해놔야하는데 이 때 memory를 쓰게되므로, CPU는 의미없는 cycle을 줄이는 것이 좋다. [Infographics: Operation Costs in CPU Clock Cycles - IT Hare on Soft.ware](http://ithare.com/infographics-operation-costs-in-cpu-clock-cycles/)  

<br>

- System state에서 control flow가 내 의도랑은 상관없이 control flow가 바뀌어진다. `내 의도와 상관없이 시스템 상태가 바뀐다.`  
1. Data arrives from a disk: 파일이 오는 동안 CPU는 다른 프로그램을 돌리다가, 파일이 다 오면 CPU는 돌리던 프로그램을 종료하고 내 프로그램으로 돌아온다. 이때 종료된 프로그램 입장에서는 자의로 종료되는 것이 아니기 때문에 좋지 않다.  
2. Data arrives from a network adapter: one drive 같은 곳에서 내 로컬로 불러오는 것이다.  
3. Instruction divides by zero: 모종의 이유에 의해서 0으로 나뉘는 경우  
4. Ctrl-C: 사용자가 현재 실행 중인 프로세스를 중단시키길 원할 때 발생한다. 시스템은 이러한 신호를 감지하고 현재 작업을 안전하게 중단시키는 등의 대응을 해야한다.  
5. System timer expires: 많은 시스템 작업들은 타이머에 의존적이다. 예를 들어, 타임아웃을 통해 시스템이 무한 대기 상태에 빠지는 것을 방지한다. 프로그램을 많이 쓰거나 하면 다른 프로그램에 넘겨줘야 할 CPU가 timer expire로 인해 이상이 생길 수 있다.  

<br>

## 1.3 Exceptional Control Flow  

<br>

- Exceptional Control Flow(ECF)는 컴퓨터 프로그램의 실행 중에 일반적인 명령 순서를 벗어나 특별한 상황을 처리하기 위해 사용되는 메커니즘을 말한다. 일반적인 control flow가 프로그램 코드에 정의된 명령어들을 순차적으로 실행하는 것을 의미한다면 ECF는 그러한 일련의 실행 흐름을 예외적인 사건이 발생했을 때 변경하도록 설계된 메커니즘이다.  
- ECF는 모든 레벨의 컴퓨터 시스템에 존재한다. 다음은 각각의 레벨에서 예외 상황을 제어하는 방법이다.  

<br>

#### Low Level Mechanisms  
- Exceptions: 프로그램 실행 중에 발생할 수 있는 예상치 못한 이벤트나 오류 상황을 처리하기 위한 메커니즘이다. `위의 5가지가 그 예라고 할 수 있겠다.`  

<br>

#### Highter Level Mechanisms  
- Process context switch: OS와 하드웨어 타이머에 의해 시행된다. Context swith가 발생하면 제어권을 OS에게 준다.  
- Signals: OS에 의해 시행된다. 정상적으로 프로그램이 종료되어도 exception이라 할 수 있다. `ex) Ctrl + C, 예외 처리가 누락된 경우`  
- Nonlocal jumps: C 런타임 라이브러리에 의해 시행된다. `setjmp(), longjmp()`  

<br>

## 1.4 Exceptions  

<br>

[OS Process, Interrupt & Exception](https://velog.io/@strurao/os-and-process)  
#### Exception  
> Exception이란 **제어권을 OS 커널에 넘겨주어 예외 상황에 대하여 대응하는 것**이다.  
- 여기서 커널이란 OS의 메모리 상주 부분이다.  
- divide by 0, arithmetic overflow, page fault, Ctrl+C 등  

<br>

`plus alpha : Virtual Address`  
- 가상 주소(Virtual Address)는 컴퓨터에서 프로세스가 사용하는 메모리 주소 공간을 가리킨다. 이는 공유되는 리소스(memory)를 쓰다보니 메모리를 나누는 것에 대한 어려움이 발생할 수도 있기 때문에 생겨난 것이다.  
- 가상 주소가 물리 주소로 변환되는 과정을 주로 하드웨어의 메모리 관리 유(Memory Management Unit, MMU)에 의해 이루어진다. 프로세스가 메모리에 접근할 때, MMU는 가상 주소를 받아 해당 주소에 매핑된 물리 주소를 찾아내고, 이를 사용하여 실제 메모리에 접근한다. 쉽게 말하면 OS가 `내가 메모리 mapping은 알아서 해줄게! 너는 알고리즘에만 신경써...` 하는 것이다.  

<br>

`plus alpha : Page fault`  
- Page fault는 가상 메모리 시스템에서 발생하는 현상으로, 프로세스가 메모리에 접근할 때 해당 페이지가 물리 메모리에 없는 경우에 발생한다. 이것이 발생하면 OS는 해당 페이지를 물리 메모리로 로드하고, 프로세스의 실행을 일시 중단하고 해당 페이지에 대한 접근을 재시도한다.  

<br>

- 이벤트가 발생했을 시 3가지 option이 있다.<img src="Docs/Pasted image 20240312155113.png">
  
- Return to I_current: 현재 명령을 다시 수행한다.  
- Return to I_next: 그 다음 명령을 수행한다.  
- Abort: 현재 프로세스를 중단시킨다.  

<br>

`plus alpha : LLM in a Flash`  
![][https://blog.kakaocdn.net/dn/cmYNvo/btsCCz6CZhu/zTn0fYSRV0xsaPGUU8dWY0/img.png]  
- LLM in a flash: 제한된 메모리에서의 효율적인 대형 언어 모델 추론 소개  
- 제한된 DRAM 용량을 가진 장치에서 대형 언어 모델 (LLM)을 실행하는 도전에 대한 고찰이다.  
- 핵심 개념  
- DRAM: 고속 데이터 엑세스를 위해 컴퓨터 및 장치에서 사용되는 메모리 유형  
- Flash Memory: 데이터 저장에 사용되는 비휘발성 메모리로, DRAM에 비해 높은 용량을 제공하지만 속도는 낮다.  
- LLMs의 희소성: 모델 매개 변수에서 많은 수의 제로 또는 거의 제로 값이 존재하는 것으로, 효율적인 데이터 처리에 활용될 수 있다.  

<br>

### 1.4.1 Exception Tables  

<br>

<img src="Docs/Pasted image 20240312160710.png">
  
- Exceptional table은 OS안에 구현되어있다. Excetion이 발생하면 그에 해당하는 exception의 번호를 찾고, 그 번호에 있는 handler 주소를 PC에 집어 넣고, 코드를 실행한다.  
- 핸들러 k가 넘어가면, exception k가 실행된다.  

<br>

### 1.4.2 Classes of Exceptions  

<br>

[Interrupt / Trap / Fault .. : 네이버블로그 (naver.com)](https://blog.naver.com/palyly/20154645190)  
- Interrupts, Traps, Faults, Aborts가 있다.<img src="Docs/Pasted image 20240312160856.png">
  
- Interrupts: 하드웨어 또는 소프트웨어에서 발생하는 외부 이벤트로, 현재 실행 중인 프로세스를 중단하고 다른 작업을 수행하도록 하는 것이다.  
- Traps: 소프트퉤어에서 발생하는 예외로, 주로 프로세스가 운영 체제의 서비스를 요청할 때 발생한다.  
- Faults: 가상 메모리 시스템에서 발생하는 예외로, 프로세스가 메모리에 접근할 때 새당 페이지가 물리 메모리에 없는 경우에 발생한다.  
- Aborts: 프로세스가 비정상적인 상황에 직면하여 실행을 중단하는 경우 발생한다. 주로 하드웨어적으로도 도저히 제어할 수 없을 때 발생한다.  
- `"Might return to current instruction"에 가볍게 별표 표시를 해놓자...`  

<br>

### 1.4.3 Asynchronous Exceptions (Interrupts)  

<br>

- 프로세서 외부에서 발생된 이벤트에 의해 발생한다.  
<img src="Docs/Pasted image 20240314151026.png">
  
- Timer interrupt, I/O interrupt from external device  
- I_next  

<br>

### 1.4.4 Synchronous Exceptions  

<br>

- 내부의 프로세스 명령에 의해 발생한다.  
- Traps:<img src="Docs/Pasted image 20240314151112.png">
  
- 나름 의도한 것  
- System calls, breakpoint traps, special instructions  
- I_next  
- Faults:<img src="Docs/Pasted image 20240314151127.png">
  
- 의도하지 않은 것  
- (아마도)복구할 수 있는 예외이다.  
- Page faults(복구 가능), protextion faults(복구 불가능), floating point exceptions  
- I_current, 핸들링이 안 되면 abort  
- Aborts:<img src="Docs/Pasted image 20240314151143.png">
  
- 의도하지 않은 것  
- 복구할 수 없는 예외이므로 프로세스를 종료 시킴으로써 그것을 처리한다.  
- Illegal instruction, parity error, machine check  
- Abort  

<br>

### 1.4.5 System Calls  

<br>

- 운영체제(OS): 컴퓨터에서 일어나는 모든 동작을 제어한다.  
- 협의의 운영체제: Kernel  
- 보편적 의미의 운영체제: Kernel + shell + File System  
- 광의의 운영체제: Kernel + Shell + Files System + System S/W + Application  
- 자원(Resource):  
- 물리적 자원: CPU, 메모리, 디스크, 터미널, 네트워크, 주변장치 등  
- 추상적 자원: 테스크, 세그먼트, 페이지, 파일, 통신프로토콜, 패킷 등  
- 커널(Kernal):   
- 프로세스 관리, 메모리 관리, IPC, 파일 시스템, 네트워킹, 디바이스 드라이버 등의 역할 담당  
- 컴퓨터의 가장 기본적인 각 장치들을 관리하고 제어하기 위한 소프트웨어이다.  
- 항상 메모리에 상주해서 컴퓨터의 각 장치들을 관리하고 제어하는 역할과 사용자들과의 의사소통을 담당한다.  
- 테스크 관리, 메모리 관리, 파일 시스템 관리, 디바이스 관리, 네트워크 관리 등을 한다.  
- System call: OS의 커널이 제공하는 서비스에 대해, 응용 프로그램의 요청에 따라 커널에 접근하기 위한 인터페이스이다.  
- $`\times 86-64`$ 시스템은 고유의 ID를 가지고 있다.<img src="Docs/Pasted image 20240314152527.png">
  

<br>

#### System Call Example: Opening File  

<br>

#### Fault Example: Page Fault  

<br>

- 기본적으로 c언어는 boundary check을 안 한다. 그래서 메모리에 접근을 해야되는지 안 해야되는지 몰라서 invalid address에도 접근할 수 있음. 이때 SIGSEGV 시그널을 보내 segfault가 났다는 것을 알려준다.  

<br>

`plus alpha: About RAM`  
<img src="https://blog.kakaocdn.net/dn/cNLgBT/btqHLeCaJzG/9jTCI1Vo0UFLDh5ZCMkMd0/img.png">
  

<br>

## 1.5 Processes  

<br>

프로세스(Process)란, 실행 중인 프로그램의 instance이다. CS에서 가장 기본이 되는 개념 중에 하나로, "program"이나 "processor"과는 다른 말이다.  

<br>

프로세스는 각 프로그램에서 다음과 같이 작용한다.  
- Logical control flow:  
- 각각의 프로그램들은 CPU를 *exclusive*하게 사용하는 것처럼 보인다.  
- *Context switching*이라는 커널 메커니즘에 의해 제공된다.  
- Private address space:  
- 각각의 프로그램들은 메인 메모리를 *exclusive*하게 사용하는 것처럼 보인다. `물리적으로는 같이 씀`  
- *Virtual memory*라는 커널 메커니즘에 의해 제공된다.  

<br>

`Plus aplha: Kernel Mechanism`  

<br>

시스템 프로그래밍에서 "커널 메커니즘"은 운영 체제의 핵심 부분인 커널이 제공하는 기능과 동작을 나타냅니다. 운영 체제의 커널은 하드웨어와 응용 프로그램 간의 인터페이스 역할을 하며, 시스템 자원 (예: 메모리, 프로세서, 입출력 장치 등)을 관리하고 프로세스 스케줄링, 메모리 관리, 입출력 관리, 보안 등 다양한 기능을 수행합니다.  
커널 메커니즘은 주로 시스템 호출(System Call)이나 인터럽트(Interrupt)와 같은 저수준 메커니즘을 포함하며, 이러한 메커니즘을 통해 응용 프로그램은 운영 체제의 서비스를 요청하고 필요한 자원에 접근할 수 있습니다. 또한, 커널은 시스템의 안전성과 보안을 유지하기 위해 권한 및 자원 관리를 담당합니다.  
시스템 프로그래머들은 커널 메커니즘을 이해하고 활용하여 운영 체제와 하드웨어 간의 효율적인 상호 작용을 구현하고 최적화하는 데 중요한 역할을 합니다.  

<br>

### 1.5.1 Multiprocessing: The Illusion  

<br>

<img src="Docs/Pasted image 20240319152149.png">
  

<br>

### 1.5.2 Multiprocessing: Reality  

<br>

<img src="Docs/Pasted image 20240314161251.png">
  
- 핑크색 박스 : 물리적 메모리  
- Register: 프로그램의 context  
- 다음은 프로그램의 context가 바뀐 것이다.<img src="Docs/Pasted image 20240319152056.png">
  
- Context switch: 돌리고 있는 프로그램이 달라지는 것.  
- Saved registers: Context를 잃지 않도록 여기다 context를 저장해놓는다. 다시 실행하면 CPU register에 이를 다시 restore한다.  

<br>

- Multicore processor는 다음과 같이 두 개가 동시에 돌아갈 수 있는 것이다.<img src="Docs/Pasted image 20240319152235.png">
  

<br>

### 1.5.3 Concurrent Processes  

<br>

- 각각의 프로세스는 logical control flow이다.  
- 물리적으로는 멈출지 몰라도 실제로 보면 동시에 돌아가는 것 처럼 보이는 것을 concurrent한 process라고 한다. 다음 그림에서는 A와 B, 그리고 A와 C가 Concurrent하게 돌아간다. 그러나 B와 C는 sequential하다. `B 끝나면 C가 돌아가니까.`<img src="Docs/Pasted image 20240319152724.png">
  

<br>

### 1.5.4 Context Switching  

<br>

- 프로세스들은 **kernel**이라고 부르는 메모리에 상주하고 있는 OS 코드에 의해 관리된다. *커널은 별도의 프로세스가 아니라 일부 기존 프로세스의 일부로 실행된다   
- Control flow는 다음과 같이 context switching을 통해 한 프로세스에서 다른 프로세스로 넘겨진다.<img src="Docs/Pasted image 20240319152816.png">
  
- 중간과정의 부산물들을 snap shot (커널이 찍어줌) 찍듯이 어딘가에 저장해야 하는 것이라고 생각하면 된다.  
- Overhead가 굉장히 크다. 왜냐면 메모리에 접근해야되는 것이기 때문에.. 따라서 우리는 이만큼의 비용을 내며 multiprocessing을 하고 있는 것이다.  
- 왜 커널이 해주냐? User는 절대로 OS process에 직접 접근하면 안되기 때문이다.  

<br>

`Plus alpha: Program memory`  
```scss
↑ High Address 
│ 
│ Stack 
│ (지역 변수, 함수 호출 정보 등) 
├─────────────── 
│ Heap 
│ (동적으로 할당된 데이터) 
├─────────────── 
│ Data 
│ (전역 변수, 정적 변수 등) 
├─────────────── 
│ Code 
│ (프로그램의 기계어 명령어들) 
│ 
↓ Low Address
```
프로그램이 실행될 때 메모리는 여러 영역으로 나누어져 사용됩니다. 이러한 영역에는 주요하게 다음과 같은 것들이 있습니다: 스택(Stack), 힙(Heap), 코드(Code), 데이터(Data) 영역 등이 있습니다.  
1. **코드(Code) 영역**: 프로그램의 기계어 명령어들이 저장되는 공간입니다. 이 영역은 실행 파일의 명령어들이 저장되는 곳으로, 프로그램이 실행되면 CPU가 여기에 저장된 명령어들을 실행합니다.  
2. **데이터(Data) 영역**: 전역 변수, 정적(static) 변수 등이 저장되는 영역입니다. 프로그램의 시작과 함께 할당되며, 프로그램이 종료될 때까지 메모리에 유지됩니다.  
3. **힙(Heap)**: 동적으로 할당된 메모리가 저장되는 공간입니다. 프로그램이 실행되는 동안 런타임에 동적으로 할당된 데이터가 저장되는 곳으로, 사용자가 직접 할당 및 해제할 수 있습니다.  
4. **스택(Stack)**: 지역 변수, 함수의 매개변수, 함수 호출 시점의 상태 등이 저장되는 공간입니다. 함수 호출 시에 생성되는 지역 변수와 함수의 호출 정보가 저장되며, 함수가 종료될 때 해당 정보는 자동으로 제거됩니다.  
시각적으로 설명하자면, 각각의 영역은 메모리의 다른 부분에 할당됩니다. 일반적으로 메모리는 가상 주소 공간으로 나타내어지며, 주소가 낮은 곳부터 높은 곳까지 커지는 방향으로 할당됩니다. 다음은 간단한 시각적 설명입니다:  

<br>

## 1.6 Process Control  

<br>

### 1.6.1 System Call Error Handling  

<br>

### 1.6.2 Error-reporting Functions  

<br>

### 1.6.3 Error-handling Wrappers  

<br>

Wrapper: 대문자로 시작하며, 실제로는 소문자로 시작하는 error-handling을 가져오는 것.  
```c
pid_t Fork(void)
{
	pid_t pid;
	if ((pid = fork()) < 0)
		unix_error("Fork error");
	return pid;
}
```

<br>

### 1.6.4 Creating and Terminating Processes  

<br>

- Life Cycle of Process: shell-ls  
- shell이 ls라는 프로세스를 띄운다 .이때 fork를 하게되는데 return값은 ls라는 프로세스의 id이다.  
- ls를 실행하게 되면, running 상태가 되고 프로세스가 종료 되면 terminate 상태가 된다. 중간에 I/O 프로세스가 있거나 하면 프로세스를 suspend했기 때문에 잠깐 stopped된다.  

<br>

#### Terminating Processes  

<br>

- kill command를 쓰면 프로세스를 죽일 수 있다. 예를들어 metrix multi를 하는데 프로세스 PID가 8000를 띄워서 열심히 곱셈을 하고 있는데 kill(utility)을 띄워서 죽이라는 signal을 보내면 PID가 그 신호를 받아서 스스로 자결한다..  
- void exit(int status)은 한 번 불리고 다시 return하지 않는다.  

<br>

#### Creating Processes  

<br>

### 1.6.5 fork Example  

<br>

[운영체제 fork() 함수란? , fork() 함수 예제 , 부모 자식 프로세스 (tistory.com)](https://code-lab1.tistory.com/39)  
- Unix 환경에서 fork() 함수는 함수를 호출한 프로세스를 복사하는 기능을 한다. 이때 parent process와 child process가 나뉘어 실행되게 된다.  
- Parent process: 원래 진행되던 프로세스, child pid 반환  
- Child process: 복사된 프로세스, 0 반환  
- fork() 함수는 프로세스 ID, 즉 pid를 반환하게 되는데 `pid_t` 이때 parent process에서는 child pid가 반환되고 child process에서는 0이 반환된다.  
- fork() 함수 실행이 실패하면 -1을 반환한다.  

<br>

```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <errno.h>
#include <string.h>

pid_t Fork(void);
void unix_error(char *msg);

int main() {
        pid_t pid;
        int x = 1;
        
        pid = Fork();
        if (pid == 0) { /* Child */
                printf("child : x=%d\n", ++x);
                exit(0);
        }
        
        /* Parent */
        printf("parent: x=%d\n", --x);
        exit(0);
}

pid_t Fork(void) {
        pid_t pid;
        
        if((pid = fork()) < 0)
                unix_error("Fork error");
        return pid;
}

void unix_error(char *msg) {
        fprintf(stderr, "%s: %s\n", msg, strerror(errno));
        exit(0);
}
```

<br>

- 위 코드에 대해 시각적으로 설명한다면 다음과 같다.<img src="Docs/Pasted image 20240321152021.png">
  
- 만약에 0이 들어가있으면 child라는 얘기  
- 만약에 2가 들어가 있으면 parent라는 얘기  
- parent는 if문을 스킵하고 x = 0을 print를 하고 exit한다.  
- child는 if문을 실행하고 x = 2를 print를 하고 exit한다.  
- 최초에는 process 하나로 시작했지만 중간에 fork를 통해 두 개가 실행이 되고 끝난다.  

<br>

### 1.6.6 Interpreting Process Graphs  

<br>

- <img src="Docs/Pasted image 20240321152550.png">
  
- Parent: a -> b -> c -> d  
- Child: a -> b -> e -> f  
- Parent와 child의 process를 그래프로 합쳐 정리하면 다음과 같다. 둘 중 뭐가 먼저오는지는 scheduling에 따라 달라질 것이다.<br><img src="Docs/Pasted image 20240321152747.png">
  

<br>

### 1.6.7 Reaping Child Processes  

<br>

- Zombie process란?:  
- 프로세스가 실행을 마쳤지만 부모 프로세스에서 종료 코드를 읽어가지 않는, 여전히 시스템의 resource를 소비하고 있는 상태이다.  
- Zombie processs는 최소한의 기본 뼈대만 유지할만큼의 적은 resource를 차지하지만, 이렇게 적은 resource일지라도 지속적으로 축적될 경우 문제가 생긴다.  
- Reaping이라는 것은 일종의 뒤처리 작업으로, child process가 종료된 경우, parent process가 reaping을 해주어야 정상적으로 종료가 된다.  
- 만약 parent process가 reap을 하지 않으면 어떻게 될까?:  
- 버려진 child process는 init provess (pid == 1)에 의해 reap된다.  
- 따라서 장기적인 process에서는 명시적인 reaping이 필요하다. (shell, server 등)  

<br>

### 1.6.8 wait: Synchronizing with Children  

<br>

- `int wait(int *child_status)`:  
- wait 함수를 통해 parent는 child를 reap한다.  
- 부모 process가 정보를 회수한 다음에야 비로소 자식 process를 공식적으로 exit하고 zombie process에서 벗어난다.  
- `wait`함수는 부모 프로세스가 특정 자식 프로세스의 종료를 기다리는 데 사용된다. 부모 프로세스가 이 함수를 호출하면, 자식 프로세스가 종료될 때까지 부모 프로세스는 파단된다. 자식 프로세스가 종료되면 부모 프로세스는 해당 자식 프로세스의 종료 코드나 상태를 가져와서 이에 대한 처리를 계속할 수 있다.  

<br>

 `Plus alpha: Wait`  
- parent는 cpu를 보내고 sleep에 들어간다. child는 exit란 시스템 콜을 호출하면서 os한테 parent한테 signal을 좀 보내줘 하는 것이다. child의 커널 자료구조에 해당하는 bit를 set해놓은 것이다. 이 process가 suspend되어있는 상태에서 ready 상태로 바꿔주는 것이다. signal handling이   

<br>

#### Another wait Example  
- `waitpid()`: 특정한 PID를 갖고있는 프로세스를 기다린다는 것  
- `WIFEXITED()`: 주어진 상태값이 자식 프로세스가 정상적으로 종료되었는지 여부를 확인한다. 즉, 자식 프로세스가 exit() 함수나 main() 함수의 반환문을 통해 종료되었는지를 확인한다.  
- `WEXITSTATUS()`: 자식 프로세스가 정상적으로 종료되었을 때 해당 프로세스가 반환한 종료 코드를 추출한다. 자식 프로세스가 정상적으로 종료되었을 때 이 메크로를 사용하여 해당 프로세스가 반환한 종료 코드를 추출할 수 있다.  

<br>

### 1.6.9 execve: Loading and Running Programs  

<br>

- `int execve(char *filename, char *argv[], char *envp[])`:  
- Obverwirtes code, data, and stack:  
- PID, open files, signal context는 유지된다.  
- 한번 호출되면 다시 return을 하지 않는다. 즉, return 값이 없다는 것이다.  

<br>

#### execve Example  

<br>

# 2. Exceptional Control Flow: Signals and Nonlocal Jumps  

<br>

## 2.1 Shells  

<br>

### 2.1.1 Linux Process Hierarchy  

<br>

<img src="Docs/Pasted image 20240326154100.png">
  
- 우리가 할 것은 MyShell을 띄우는 것  
- cmd에서 받은 걸 parsing을 한 다음에 execve에서 실행시켜주는 것이 shell이 하는 것이다.  

<br>

### 2.1.2 Shell Programs  

<br>

- while문을 돌면서 cmdline을 받고 cmd_handling을 하고 eval을 하는 것  
```c
int main()
{
	char cmdline[MAXLINE];
	while(1) {
		printf("> ");
		Fgets(cmdline, MAXLINE ,stdin);
		if(feof(stdin))
			exit(0);
		
		eval(cmdline);
	}
}
```
- `Fgets(cmdline, MAXLINE, stdin)`: 사용자로부터 표준 입력(stdin)을 통해 명령을 읽어들인다. `MAXLINE`보다 길 경우 자르게 된다.  
- `if(feof(stdin))`: 만약 입력이 EOF(End Of File)라면 프로그램을 종료한다. 일반적으로 Ctrl+D를 누르면 EOF가 입력된다.  

<br>

#### Simple Shell eval Function  

<br>

- 이 간단한 프로그램의 문제점은 다음과 같다.  
- Background jobs: 종료되지 않고 끝나 zombie가 될 수 있다.  

<br>

`Plus alpha: '&'  
- `&`: 백그라운드(background)에서 명령을 실행하라는 의미이다. 즉, 해당 명령은 실행되지만 터미널이 차지되지 않고 다른 작업을 할 수 있다. 이 경우에는 해당 결과가 터미널에 직접 출력되지 않고 백그라운드에서 실행되기 때문에 결과를 볼 수는 있지만, 터미널 프롬프트가 다시 사용 가능한 상태로 바로 돌아온다.  

<br>

### 2.1.3 ECF to the Rescue!  

<br>

- Exceptional control flow를 통해  

<br>

## 2.2 Signals  

<br>

#### Signal  
 >시스템에서 어떠한 타입의 event가 발생했을 때 프로세스에게 알리는 small message  

<br>

- Exception과 intterrupt를 처리하는 방식과 굉장히 비슷하다.  
- Signal은 커널이 보낸다. ($`Kernel \rightarrow Process`$ or $`P_B \rightarrow _{\text{요청}} Kernel \rightarrow _{Signal} P_A`$)  
- Signal의 type은 small int(1-30)이다.  
- Signal의 정보는 오직 그것의 `ID`와 `도착했다는 사실`이다.  

<br>

| ID  |   Name   | Default Action |           Corresponding Event           |  
| :-: | :------: | :------------: | :-------------------------------------: |  
|  2  |  SIGNT   |   Terminate    |            User typed ctrl+c            |  
|  9  |   SIG    |   Terminate    | Kill program (cannot overide or ignore) |  
| 11  | SIGSEGV  |   Terminate    |         Segmentation violation          |  
| 14  | SIGALRM  |   Terminate    |              Timer signal               |  
| 17  | SIGCHILD |   **Ignore**   |       Child stopped or terminated       |  

<br>


<br>

#### Sending a Singnal  
> Kernel은 signal을 destination process로 전달하는 데, 이때 destination process의 context의 state를 업데이트 한다.  

<br>

#### Receiving a Signal  
> Kernel이 destination process에게 signal handle를 하라고 강제하면 그것을 signal이 received 되었다고 한다.  

<br>

- Signal을 받았을 때 어떻게 react를 할지는 다음과 같은 종류가 있다.  
- Ignore: signal을 무시한다. 즉, 아무것도 하지 않는다.  
- Terminate  
- Catch  

<br>

#### Pending and Blocked Signals  
> Pending: Signal이 보내졌지만 아직 receive하지 않은 상태  
> Block: Block된 signal은 deliver 될수는 있지만, unblocked되지 않는 한 received되지는 않는다.  

<br>

- Pending에서의 kernel의 동작:  
- signal of type k is delivered -> Kernel sets bit k  
- signal of type k is received -> Kernel clears bit k  
- pending이라는 bit vector와 block 이라는 bit vector가 있고, 처음에는 모두 0으로 초기화가 되어있다. 사용자는 block의 bit vector를 masking할 수 있다. 사용자는 pending & ~block이여야만 signal을 받아서 동작을 하겠다는 이야기이다. 만약 pending이 1인데, block이 1이라면 signal을 받되, 아무것도 하지 말라는 것이다. `sigprocmask() 라는 함수가 있다.`  
- Kernel는 pending과 blocked bit vector들을 각 프로세스의 context에 저장해놓는다.  

<br>

### 2.2.1 Sending Signals  

<br>

#### Process Groups  
<img src="Docs/Pasted image 20240404151322.png">
  

<br>

#### /bin/kill Program  

<br>

#### Sending Signals from the Keyboard  
- `ctrl-c`(`ctrl-z`)는 커널로 하여금 모든 fg 프로세스 그룹에 SIGNIT(SIGTSTP)를 보내도록 한다.  
- SIGNIT(`ctrl-c`): terminate each process  
- SIGTSTP(`ctrl-z`): stop(suspend) each process  

<br>

### 2.2.2 Recieving Signals  

<br>

> 커널이 exception handler로부터 반환값을 받아 control을 process p에 넘겨줄 준비가 되었다고 가정해보자.  
- 커널은 다음과 같은 타이밍에 bit vector를 확인해서 singal을 확인한다.<img src="Docs/Pasted image 20240404152417.png">
  
- 그다음 커널은 `pnb = pending & ~blocke`를 계산한다. pending이 1이고 blocked가 0일 때만 실행하라는 것이다.  
```c
if (pnb == 0) {
	// do nothing
	// pass control to next instruction in the logical flow for p
}
else {
	// k = 1이면 p로 하여금 signal k를 recieve하게 한다.
	// signal의 receipt는 p로 하여금 어떤 action을 trigger한다.
	// k가 1일 동안 계속 반복한다.
	// pass control to next instruction in the logical flow for p
}
```

<br>

#### Default Actions  
> Signal의 reciept가 process로 하여금 action하게 하는 기본적인 종류로는 다음과 같은 것들이 있다.  
- Process *terminates*  
- Process *stops* until restrated by a SIGCONT signal  
- Process *ignores* the signal  

<br>

`plus alpha: chile kill and wait`  
child가 다섯 번 죽으면 wait도 다섯 번 호출해야한다.  
더 자세한 것은 fork 14, 15참고  

<br>

#### Instaling Signal Handlers  
> handler_t *signal(int signum, handler_t *handler);  
- 이벤트가 발생했을 때 handler가 user defined function으로 바뀌는 것이다.  
- 이런 식으로 우리는 signal을 심을 수 있다.  

<br>

#### Signals Handlers as Concurret Flows  
> Signal handler또한 각 process가 concurrent 하게 돌아가는 것처럼 보이듯이 process와 같이 돌아가는 것처럼 보이지만 사실 따로 돌아가고 있는 것이다.  
- Signal handler를 concurrent flow 관점으로 보고 시각화하면 다음과 같다.<img src="Docs/Pasted image 20240404155300.png">
  

<br>

#### Nested Signal Handlers  
> Handler 또한 다른 handler들에 의해 interrupted 될 수 있다.  
- 원칙적으로는 다음과 같은 flow가 가능하다는 것이다.<img src="Docs/Pasted image 20240404155522.png">
  

<br>

#### Blocking and Unblocking Signals  
- Implicit blocking mechanism  
- Explicit blocking and unblocking mechanism  
- Supporting functions  

<br>

### 2.2.3 Safe Signal Handling  

<br>

- Handler는 사실 main program과 concurrent 하고 같은 global data structures를 공유하기 때문에 굉장히 복잡한 매커니즘이지만 효율성을 위해서 다뤄야할 필요가 있다.  

<br>

#### Guidelines for Writing Safe Handlers  
> 위와 같은 문제들 때문에 안전한 handler를 짜기 위한 Guideline들이 존재한다.  
- G0: Keep your handlers as simple as possible  
- G1: Call only async-signal-safe functions in your handlers  
- printf, sprintf, malloc, exit 등은 handler 안에서 쓰지 마라! (un safe)  
- G2: Save and restore errno on entry and exit  
- errno는 다른 변수에 저장했다가 복구하는 식으로 써라  
- G3: Protect accesses to shared data structures by temporarily blocking all signals.  
- G4: Declare global variables as volatile  
- 값이 register에 있지 않고 memory에 남아있다. 따라서 memory 연산을 할 때 global 변수들을 최대한 volatile로 선언을 해라.  
- G5: Declare global flags as volatile sig_atomic_t  

<br>

`plus alpha: volatile`  
말 그대로 따지면, 변수를 휘발성 메모리에 만들어라 라는 뜻이다.  

<br>

#### Asnc-Signal-Safety  
> Signal을 받았을 때 함수가 중간에 쪼개지지 않는 것이 async-signal-safe 하다고 한다. (reentrant or non-interruptible by signals)  
- 이를 위해 SIO(Safe I/O library)를 제공해준다. `csapp.c에 있음`  
```c
ssize_t sio_puts(char s[]) /* Put string */ 
ssize_t sio_putl(long v) /* Put long */ 
void sio_error(char s[]) /* Put msg & exit */
```

<br>

#### Example 37p.: Correct Signal Handler  
- handler가 2번 밖에 뜨지 않아 두번 reaped가 되고 hanging이 된다.  
- child는 5번의 signal을 보냈지만 handler는 2번밖에 안 뜨는 상황이다.  
- 문제점, child의 무작의 exit:  
- child가 무작위로 exit하는데 처음에 SIGCHILD를 받아서 child_handler가 실행되고 있는 와중에 그 다음 SIGCHILD가 도착하면 그 signal은 무시된다. 그렇게 3개가 무시되고 2개만 출력, 그리고 `ccount = 3`이어서 while을 탈출할 수 없는 것이다.  

<br>

#### Example 38p.: Correct Signal Handler  
- while loop을 돌면서 zombie가 있는지 확인하면서 reaping을 해준다.  
- handler를 실행하는 동안 쌓여있던 signal을 해소하는 것이다.  
- handler가 어느 child를 대상으로 돌지는 알 수 없다.  

<br>

### 2.2.4 Portable Signal Handling  

<br>

signal handler를 등록하면 signal 을  
- errno == EINTR: read(slow system call) 가 돌아가는 중간에 system call이 발생하면 call이 실패하는데 이를 방지하기 위해 system call을 다시 불러오는 것이 있다.  
- 어떤 특정 signal은 blocking(set mask)이 안된다. 그런 경우에는 `sigaction()`을 쓰자.  
- sigemptyset: 세번째 해결  
- SA_RESTART: 두번째 해결  
- if: 첫번째 해결  

<br>

#### Synchronizing Flows to Avoid Races  
- correct signal handler와 전체적인 로직은 비슷하다.  
- `addjob(pid)`, `deletejob(pid)`라는 로직이 추가되었다. main에서 넣어준 pid를 signal handler가 queue에서 dequeue를 하겠다는 것이다. 결국 event가 언제올지 모르니까 비동기적으로 하겠다는 이야기이다. 사실은 굉장히 심플한 코드이다.  
- `waitpid()`의 첫번째 인자에 -1을 넣으면 아무 프로세스나 종료하겠다는 것이다.  
- 문제점, addjob이 먼저냐, pid = fork가 먼저냐  
- parent가 먼저 add를 해놓고 signal을 받으면 delete를 하겠다. 라고 생각하고 짜는데 이러한 순서가 `pid = fork -> date -> SIGCHILD`가 addjob보다 먼저 발생이 되서 예상치 못한 문제가 발생할 수 있다.  

<br>

#### Corrected Shell Program without Race  
- add나 delete를 하기 전까지 다른 동작은 안할거라고 하고 수행된 후에 옛날걸로 복구를 해놓고 짜는 방식  
- 으로 하는데도 오류가 발생한다!  
- OS가 스케쥴링을 하기 때문에 이러한 문제가 발생하는 것이다. 내가 짠 코드가 중간에 언제든지 선점당할 수있다는 것이다. 뺏기는 단위는 Instruction 단위이다.  
- $`I_{prev} -\text{premption (CPU 제어)}-> I_{curr} -\text{premption (CPU 제어)}-> I_{next}`$  
- 결국 masking하는 시점을 잘 조절해야한다.  
- `&mask_one`, `Sigprocmask` fork 전부터 fork 다음까지 signal을 받지 않는다(deliver하되 received하지 않는것).  
- If 문을 기점으로 child는 unlock 한 다음에 execve 실행, parent는 모든걸 block 한 다음 addjob를 수행하고 그 다음에 SIGCHILD를 unblock한다. (handler 실행 -> delete 실행)  

<br>

#### Race란?  
- ..  
- Race를 해결하는 방법은 order를 정해주는 것(직렬화)인데, 과하면 concurrency(병렬화)가 무너질 수 있기 때문에 적절히 조절할 필요가 있다.  

<br>

#### *Explicitly Waiting for Signals (test?)*  
> background process가 종료되었다는 걸 내가 어떻게 알 수 있을까?  
- child가 reaping이 되면 pid(0)이 pid(!=0)이 되면서 while loop이 종료된다. 하지만 이는 CPU 관점에서 그리 좋지는 않다. 이를 위한 option이 두 가지가 있다.  
```c
while (!pid) { pause(); }
```
signal이 올 때까지 잠든다.  
signal handler와 main 사이에 race가 일어날 수 있다.  
잠들기 전에 SIGCHILD가 들어와버리면 영원히 잠들 수 있다.  
ctrl+c 같은게 들어와도 예상과는 달라질 수 있다.  

<br>

```c
while (!pid) { sleep(1); }
```
일정 시간동안 잠들었다가 일어난다.  
sleep에 전달하는 인자 값에 따라 pid의 확인 빈도가 달라진다.  

<br>

#### *Waiting for Signals with `sigsuspend`*  
> 호출한 프로세스를 중단시킴과 동시에 시그널 셋을 블록시킨다.  
- `int sigsuspend(const sigset_t *mask)`는 기능적으로는 다음과 같은 코드의 역할을 한다. atomic instruction이다.  
```c
sigprocmask(SIG_BLOCK, &mask, &prev);
pause();
sigprocmask(SIG_SETMASK, &prev, NULL); // &prev: child만 안 받겠다. 로 바뀐다.
```

<br>

`plus alpha: atomic operation`  
기능적으로 분할할 수 없거나 분할되지 않도록 보증된 조작이다. 만일 중지되면 동작 개시 직전의 상태로 시스템을 복귀시킬 것을 보증하는 복구(백업과 복원) 기능이 제공된다.  

<br>

# 3. System-Level I/O  

<br>

## 3.1 Unix I/O  

<br>

file을 byte들의 sequence로 볼 수 있다.  
lseek()로 내가 read나 write할 offset을 임의로 옮길 수 있다.  

<br>

### 3.1.1 File Types  

<br>

file이라는 건 세 가지로 표현할 수 있다.  
filename : 절대 경로 (absolute path)  
node: 번호  
fd  

<br>

- 각 파일에는 시스템에서의 역할을 나타내는 유형이 있다.  
- Regulat file: 임의의 data 포함  
- Directory: 폴더의 개념  
- Socket: 네트워크 상의 다른 process와 통신하기 위한 파일  

<br>

- 이외의 파일들:  
- Named pipes(FIFOs)  
- Symbolic links: 바탕화면 바로가기 등등  
- Character and block devices: Reyboard, SSD 등  

<br>

#### Regular Files  

<br>

### 3.1.2 Directory  

<br>

#### Directory  
> file의 filename을 가리키는 link 배열을 포함한다.  
- `.`(자기 자신의 link), `..`(부모 디렉토리의 link)  
- `mkdir`, `ls`, `rmdir`(빈 디렉토리 삭제)  

<br>

# 4. Network Programming: Part I  

<br>

#### Reference  
[SP - 3.1 Network Programming (1) (velog.io)](https://velog.io/@junttang/SP-3.1-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EA%B0%9C%EB%A1%A0)  

<br>

## 4.1 Client-Server Transaction  

<br>

거의 모든 network application들은 client-server 구조이다.  
여러개의 client가 하나의 server에 존재한다고 생각하면 된다.  

<br>

#### Hardware perspective  
> Network adapter를 자세히 보자.  
<img src="Docs/Pasted image 20240418152032.png">
  
- main memory에서 copy가 한 번 일어나고  
- DMA를 통해서 dist condroller가 데이터를 가져감 (쓰기)  
- CPU가 가져오는 게 아니라 DMA가 가져오는 것  

<br>

#### Computer Networks  
> Network는 계층적인 구조를 띤다. Internetwork는 이 network를 상호 연결하고 있는 네트워크 집합이다.  
- SAN(System Area Network): Cluster, Room  
- LAN(Local Area Network): Building, Campus  
- WAN(Wide Area Network): Country, World  

<br>

#### SAN: Ethernet Segment  
> Ethernet segment는 hub에 wire(twisted pair)로 연결된 호스트 모음으로 구성된다.  
- (층 또는 빌딩 커버)  
- MAC address: 각각의 ethernet 어댑터는 고유의 48-bit 주소를 가지고 있다.  
- Bridged ethernet segment: 각각의 ehternet segment들을 더 큰 대역폭을 가지고 있는 bridge로 연결한다. (빌딩 또는 campus 커버) 이를 개념적으로 LAN이라고 한다.  

<br>

#### LAN: Bridged Ethernet Segment  
> 단순화를 위해 hub, gridge, wire는 단일 wire에 연결된 host 모음으로 표시되는 경우가 많다.  

<br>

#### WAN: internet  
> 연결된 network를 internet이라 부른다. `Internet(대문자)이 아니다.`  
- 여러 개의 호환되지 않는 LAN은 router라는 특수 컴퓨터에 의해 물리적으로 연결될 수 있다.  

<br>

어떻게 호환되지 않는 LAN이나 WAN사이에서 bit를 교환할까?  
-> Protocol SW on each host and router  

<br>

#### internet Protocol  
> Naming scheme과 delivery mechanism을 제공한다.  
- naming scheme:  
최소 1개 이상의 고유 host address를 가지게 해준다.  
- delivery mechanism:  
packet{header, payload}을 어떻게 보낼지 정의해준다.  
- Header: packet size, source, dest address  
- Payload: Source host로부터 보내진 data bits  

<br>

<img src="Docs/Pasted image 20240430152446.png" width="600">
  
protocol software 부터 kernel의 영역,  
LAN adapter는 물리적 영역  
internet packet에가 FH1을 붙여서 LAN1 frame을 만들고 network adaptor로 보낸다.  
router의 protocol SW에서 LAN2의 FH2로 바꿔준다.  

<br>

`Other issue: 생길 수 있는 추가 문제들`  
만약 각 LAN의 frame size가 다르다면? -> segmentation  
frame dest LAN은 어떻게 설정하지?  
router가 망가지거나 link 회선이 망가지면 이를 어떻게 알릴것인가?  
packet loss가 일어난다면?  
-> CSE4175에서 다룰것이다.  

<br>

## 4.2 Global IP Internet & TCP/IP Protocol  

<br>

#### Global IP Internet  
> 가장 유명한 internet의 예시이다. TCP/IP protocol family를 기반으로 한다.  

<br>

#### IP(Internet Protocol)  
> Host 식별을 위한 Naming Scheme(Host Address)을 제공한다.  
- host간의 unreliable data packet delivery를 제공한다. data의 손실과 duplicate packet 문제를 막지 못한다는 것이다.  

<br>

#### UDP(Unraliable Datagram Protocol)  
> IP를 이용해 process 간의 unreliable packet delivery를 제공한다.  
- 불안정하긴 하지만, 굉장히 속도가 빠르다. data가 손실되어도 크게 문제되지 않는 상황에서 유용하게 쓰인다.  

<br>

#### TCP(Transmission Control Protocol)  
> IP를 이용해 connection을 가진 process 간의 reliable byte stream을 제공한다.  
- data의 손실과 duplicate packet 문제를 해결할 수 있다.  

<br>

<img src="Docs/Pasted image 20240430153317.png" width="600">
  

<br>

## 4.3 IP Address & Internet Domain Name  

<br>

#### IP Address  
> Global IP Internet, 줄여서 Internet은 총 32bit의 IP 주소 집합에 Host를 맵핑시킨다.  
- 이 IP 주소는 Internet Domain Names와 일 대 다수 혹은 다수 대 일로 맵핑이 가능하다.  

<br>

하나의 Internet Host의 process는 다른 Internet Host의 process와 **connection**을 이루어 소통할 수 있다.  

<br>

#### Aside: IPv4 and IPv6  
- IPv4: 32bit  
- IPv6: 128bit  

<br>

### 4.3.1 IP Addresses  

<br>

#### IP address struct  
- network byte order: big endian byte order  
- host byte order: little endian byte order  
-> conversion 해주어야 한다.  
- host -> network: `unit32_t htonl()`, `unit16_t htons()`  
- network -> host: `unit32_t ntohl()`, `unit16_t ntohs()`  
#### Dotted Decimal Notation  
> 0x8002c2F2 = 128.2.194.242  
- IP address ->  dotted decimal format: `getaddrinfo()`, `getnameinfo()`  

<br>

### 4.3.2 Internet Domain Names  

<br>

#### DNS(Domain Naming System)  
> Internet은 IP address와 domain name의 mapping 관계를 세계구급의 분산 데이터베이스 DNS에 보관한다.  
- 프로그래머 관점에서, DNS 데이터베이스는 수많은 Host Entry의 집합이다.  
각 Host Entry는 IP 주소와 domain name의 mapping 관계를 정의한다.  
- DNS 데이터베이스에는 수많은 도메인 list가 있고, 우리는 이 DNS에서 제공하는 서비스를 받아서 프로그래밍을 하면된다.  

<br>

caching DB도 있고, DNS data를 분산해서 가지고 있는 DB도 있고..  

<br>

#### nslookup  
> DNS mapping 관계를 탐색할 수 있다.  

<br>

### 4.3.3 Internet Connections  

<br>

#### Internet Connections  
> Internet에서, Client와 server는 'Connection'을 통해 byte stream을 주고받을 수 있다.  
- Point-to-Point: Process를 각각 point로 보아, process의 pair끼리 connection을 형성한다.  
- Full-Duplex: Data는 connection의 양방향에서 동시에 전송될 수 있다.  
- Reliable: Source에서 보낸 data가 dest에 도달할 때, 전송할 때의 byte stream 순서 그대로 유지되어 전송된다. (TCP is Reliable)  

<br>

#### Socket  
> Connection의 양 끝 말단 Endpoint를 socket이라 한다.  
> Socket의 주소는 'IP adress: Port'의 pair로 이루어진다.  

<br>

<img src="Docs/Pasted image 20240430160150.png" width="400">
  
- Port:  
16bit 정수로, process를 식별하는 데 사용된다.  
- Ephemeral Port: OS Kernel이 Client의 connection request가 발생할 때 알아서 자동적으로 할당하는 포트  
- Well-Known Port: 서버에 의해 제공되는 서비스와 연관된 미리 예약된 포트, /etc/services에 명시되어 있다.  
`echo server: 7/echo, ssh server: 22/ssh, email server: 25/smtp, Web server: 80/http`  

<br>

Connection은 양쪽 Endpoint Socket의 Socket Address의 Pair로 식별한다.  
<img src="Docs/Pasted image 20240430160551.png" width="600">
  

<br>

# 5. Network Programming: Part II  

<br>

## 5.1 Socket Interface  

<br>

#### Socket Interface  
> UNIX I/O를 사용해 Network Application을 만드는데 쓰이는 Set of System-Level Functions(System Calls)  
- Socket interface를 이용해 network application을 작성할 수 있다.  

<br>

#### Socket  
> Kernel 입장에서 Socket은 connection의 Endpoint이다.  
> Application 입장에서 Socket은 File Descriptor이다.  
- Client와 Server라는 두 Host는 Socket Descruptor를 이용해 서로에게 읽고 쓴다.  
- 우리가 쓸 Socket Address의 구조는 다음과 같다.  
```c
struct sockaddr { 
	uint16_t sa_family; 		/* Protocol Family, protocol 정보 */ 
	char sa_data[14]; 			/* Address Data */ 
};
```
- 실제 Internet을 위한 Socket Address의 구조는 좀 더 구체적이다. `IPv4 기`  
```c
struct sockaddr_in { 
	uint16_t sin_family; 			/* Protocol Family (항상 AF_INET) */ 
	uint16_t sin_port; 				/* Port 넘버 (network byte order) */ 
	struct in_addr sin_addr; 		/* IP 주소 (network byte order) */ 
	unsigned char sin_zero[8]; 		/* 크기 유지를 위한 공간 */ 
};
```

<br>

<img src="Docs/Pasted image 20240502151430.png">
  
- binding: kernel에게 struct를 알려주는 것.  
- listen: 뭔가의 connection request가 kernel로 오는 것을 계속 기다리고 있는 것  
- accept: 받아들일지 말지 판단  
- EOF가 날라오기 전까지 write와 readlineb를 반복한다.  

<br>

`plus alpha: real Socket Interface`  
- kernel에서는 TCP manager라는 함수가 돌아가고 있다. connection request를 받아서 FIFO queue에 쌓는다.  
- accept 판단을 하던 도중 request가 도달하면 어떻게 될까? -> TCP manager queue에 들어간다.  

<br>

### 5.1.1 Socket Interface Functions  

<br>

#### getaddrinfo  
> Hostname, host address, port, service name을 받아와 Socket Address struct에 반영하는 함수이다.  
- Pros:  
- Reentrant하다. Thread Programming에서 사용해도 문제가 없다는 것이다.  
- Protable한 'Protocol Independent Code'를 작성할 수 있다.  
- Cons:  
- 사용법이 복잡하다.  
- 그나마 사용법이 정형화 되어있어서 할만하다.  
```c
int getaddrinfo(
const char *host, 				/* Hostname 또는 Address */
const char *service, 			/* Port 또는 Service 이름 */
const struct addrinfo *hints,	/* 옵션 */
struct addrinfo **result		/* Output 연결리스트 */
); 

void freeaddrinfo(struct addrinfo *result); 	/* Free 연결리스트 */
```
- 주소 정보에 대한 Linked List를 반환한다.  
- linked list의 각 node에는 socket address struct에 대한 정보와, socket interface 함수들에 대한 인자 정보들이 담긴다.  
- Linked List usage:  
- Client: Linked List를 순회하며 Socket에 대한 호출과 연결이 성공할 때까지 각 Socket Address를 탐색한다.  
- Server: Linked List를 순회하며 Socket에 대한 호출과 Binding이 성공할 때까지 각 Socket Address를 탐색한다.  

<br>

#### getnameinfo  

<br>

### 5.1.2 Example: Echo Server  

<br>

#### accept Illustrated  
실제 통신하는 것은 conddfd(4), 즉 file descript 4번과 통신을 하는 것이다.  
이때 fork를 띄워서 concurrent하게 돌아갈 수 있게 한다. 이때 listenfd(3)를 이용해 Request를 queueing(보관) 해놓는 것이다.  
Client가 새로 request를 보낼 때마다 conddfd(4 + n)을 해서 accept를 해주는 것이다.  

<br>

# 6. Concurrent Programming  

<br>

#### Introduction  
> Concurrent Programming이란, 앞서 Process와 Signal에서 다룬 Concurrent Flow를 떠올리면 되는데, 여러 프로그램과 Flow가 동시적으로 수행되는 상황을 다루는 프로그래밍을 의미한다.  

<br>

인간은 본능적으로 순차적인 프로그래밍에 익숙하기 때문에 Concurrent Programmning을 하기란 어렵다. 우리가 예측하지 못하는 흐름을 가질 때가 많기 때문에 제어하기 어렵다.  
다양한 *Possible Flow*를 다뤄야 한다.  

<br>

#### Concurrent Programming is Hard!  
> 우리는 Races, Deadlock, Livelock/Starvation/Fairness와 같은 어려움을 맞이할 수 있다. 이중 일부을 본 강의에서 커버할 것이다.  
- Races:  
프로그래밍의 결과가 시스템의 임의 스케줄링 결정에 의해 좌우된다.  
`who gets the last seat on the airplane?`  
- Deadlock:  
부적절한 Resouce Allocation이 앞선 프로세스들의 진행을 막을 수 있다.  
`traffic gridlock`  
- Livelock/Starvation/Fairness:  
외부 이벤트 and/or 시스템 스케줄링 결정은 하위 프로세스 진행을 막을 수 있다.  
`people always jump in front of you in line`  
- Livelock: 경쟁의 원인  
- Starvation: 한 번도 Winner가 되지 못하는 것  
- Fairness: 공정하게 경쟁을 시작했음에도 결과는 그러지 못하는 것  

<br>

#### Iterative Servers  
> 'Not Concurrent Server'(Iterative Server)의 문제점 `echo server`  

<br>

<img src="https://velog.velcdn.com/images/junttang/post/fc7b985f-f499-4b10-ad96-1bd402741a37/image.png" width="600">
  
- 앞선 Connection이 끊기기 전까진 새로운 accept를 하지 못해 불가피하게 기다리는 시간이 발생한다.  
- Client2는 Server와 Connection이 맺어지진 않았지만, Server에게 Message를 write할 수는 있다.  
connect는 Request만 보내고 바로 수행이 끝난다. 그말은 즉슨, *반대편에서 Request를 받든 말든, 일단 동작을 할 수 있는 것이다.*  

<br>

>> Connection을 빌드하지 못한 Client는 무작정 대기해야 하는 문제가 발생한다.  
>> Server는 본연의 역할을 제대로 수행하지 못한다.  

<br>

##### TCP listen backlog  
Server의 OS Kernel 내 Network Stack에 있는 TCP Manager에 Request가 queuing된다. accpet는 되지 않고 말이다.  
`Client가 connect 함수를 통해 Connection Request를 보냈을 때, Server에서 이를 accept하지 않아도 Client는 데이터 통신을 시도할 수 있다.`  

<br>

##### rio_written returns  
Request는 보냈는데, accept가 이뤄지지 않은 상황에서 Client가 Data를 보내면 Server 쪽의 TCP Manager에 Data가 버퍼링된다.  
`Client2 쪽의 connect와 rio_written 함수는 return을 마친 것이다.`  

<br>

##### rio_readlineb blocks  
Server는 Connection이 빌드된 Client를 제외하고는 다른 Client의 Data를 Concume하지 못하고 있는 것이다.  
`그러나 양쪽의 rio_readlineb는 blocking되어 있다.`  

<br>

## 6.1 Concurrent Server Methodology  

<br>

<img src="https://velog.velcdn.com/images/junttang/post/8b6b9543-d9be-4f3e-b8c4-3b81f27a4e03/image.png" width="550">
  

<br>

#### Process-based  
> Process 여러 개 <- fork()  
> `Server process에서 fork를 이용해 여러 복제 process를 띄우는 방식`  

<br>

#### Event-based  
> Process 한 개, Concurrency issue가 없다.  
> `하나의 process에서 일련의 event 처리 루틴을 이용해 동시성을 제공하는 방식`  

<br>

#### Thread-based  
> Process 한 개, Multiple Execution Flow  
> `하나의 process에서 여러 Thread를 만들어, 여러 Execution Flow를 운영하는 방식`  

<br>

|               Process-based               |           Event-based            |             Thread-based             |  
| :---------------------------------------: | :------------------------------: | :----------------------------------: |  
|     Kernel이 자동적으로 복수의 Logical Flow 운영     |  프로그래머가 직접 복수의 Logical Flow 운영   |  Kernel이 자동적으로 복수의 Logical Flow 운영   |  
| 각각의 flow가 자신의 private address space를 가진다. | 모든 flow가 같은 address space를 공유한다. |   모든 flow가 같은 address space를 공유한다.   |  
|                                           |         I/O Multiplexing         | Process-based + Event-based (Hybrid) |  

<br>

## 6.2 Process-based Concurrent Server  

<br>

<img src="https://velog.velcdn.com/images/junttang/post/4172b277-49b1-4248-a763-3d91d5b3d28e/image.png" width="500">
  

<br>

Client N개가 Server에게 Request를 보내면, Server에서는 N개의 Child Process가 생성되나.  

<br>

Child는 listenfd를 안 쓰고  
Parent는 connfd를 안 쓴다.  
꼭 Close로 닫아주자. memory leak를 방지하기 위해서  

<br>

잘 복제하고 문 잘 닫고 잘 열고.. `fork(), Close`  

<br>

<img src="https://velog.velcdn.com/images/junttang/post/0e19da12-52dd-44c0-887c-6e0f541a9eff/image.png" width="500">
  
- 각 Child끼리 어떤 space나 state도 공유하지 않는다.  
- Parent는 fork 이후 바로 connfd를 close 해야 하고,   
- Child는 fork 이후 바로 listenfd를 close 해야 한다.  

<br>

#### Issue  
- Listening Server는 zombie child를 reap 해줘야 한다.  
- Parent는 복제된 connfd를 꼭 close 해줘야 한다.  

<br>

#### Pros & Cons  
- + 여러 Connection을 cocurrent하게 처리할 수 있다.   
- + 깔끔한 Sharing Model을 가질 수 있다. `Open File Table 제외 다른 것은 공유하지 않음`  
- + 이론이 단순하고 직관적이다.  
- - Process Control을 위한 Overhead가 많다. `시간적 overhead`  
- - 오로지 IPC(Inter Process Communication)을 통해서만 각 Process가 통신할 수 있다. `FIFO 형대의 pipeline 같은 것`  

<br>

## 6.3 Event-based Concurrent Server  

<br>

#### I/O Multiplexing  
> Server가 listenfd와 connfd Array를 지속적으로 체크하면서 Pending Input이 있으면 accept를 하고, $`connfd_i`$ 쪽에 Pending Input이 있으면 서비스 제공을 하는 메커니즘이다.  
>> *이때 Pending Input은 곧 '이벤트가 도달했음'을 의미한다.*  

<br>

<img src="Docs/Pasted image 20240509153237.png">
  
5 client가 connection을 맺은 상태  
초록: active인데 pending이 있는 것  
inactive인데 pending이 있는 것은 있을 수 없다.  

<br>

```c
int select(int n, fd_set *fdset, NULL, NULL, NULL);
/* Active(Ready) Descriptor의 개수를 반환한다. 없으면 -1(에러)이다. */

// FD_ZERO : fdset에 있는 모든 비트를 0으로 처리한다.
// FD_CLR : fdset의 특정 k번 비트(fd)를 Clear한다.
// FD_SET : fdset의 특정 k번 비트(fd)를 Set(Turn On)한다.
// FD_ISSET : fdset의 특정 k번 비트(fd)가 Set인지 확인한다.
//   ~> FD_ISSET은 Pending Input 확인 시 유용하게 쓰인다. ★
```

<br>

```c
void read_cmdline(void) {
	char buf[MAXLINE];
    Fgets(buf, MAXLINE, stdin);
    printf("%s", buf);
}

/* Server with I/O Multiplexing (Event-based Concurrent Server) */
int main(int argc, char **argv) {
	int listenfd, connfd;
    socklen_t clientlen;
    struct sockaddr_storage clientaddr;
    fd_set read_set, ready_set;			// fd_set Type의 두 비트 벡터를 만든다.
    
    if (argc != 2)
    	unix_error("Usage Error!\n");
    
    listenfd = Open_listenfd(argv[1]);	// listen까지의 작업 수행. 3번이 넘어왔다고 하자.
    
    /* 서버 입장에서, stdin과 listenfd의 Pending Input을 확인할 준비를 한다. */
    FD_ZERO(&read_set);					// read_set은 모두 FALSE로 초기화하자.
    FD_SET(STDIN_FILENO, &read_set);	// read_set에 stdin을 활성화한다.
    FD_SET(listenfd, &read_set);		// read_set에 listenfd도 활성화한다.
    
    while(1) {
    	ready_set = read_set;			// ready_set은 read_set을 카피한다.
        /* ready_set에서 TRUE, 즉, Pending Input이 있는 element를 뽑아내겠다. */
        Select(listenfd + 1, &ready_set, NULL, NULL, NULL);
        
        if (FD_ISSET(STDIN_FILENO, &ready_set))		// stdin에서 이벤트 발생 시
        	read_cmdline();							// 타이핑을 읽고 처리한다.
        if (FD_ISSET(listenfd, &ready_set)) {		// listenfd에서 이벤트 발생 시
        	clientlen = sizeof(struct sockaddr_storage);	// Accept한다!
            connfd = Accept(listenfd, (SA *)&clientaddr, &clientlen);
            echo(connfd);							// 서비스를 제공한다.
            Close(connfd);							// 사용한 connfd File은 종료!
        }
    }
}
```
<img src="Docs/Pasted image 20240509154043.png" width="300">
<img src="Docs/Pasted image 20240509154218.png" width="300">
  

<br>

Select 함수가 readt_Set이라는 것이 있는데 이 함수가 호출되는 순간 process가 sleep(blocking)되고 event(pending request, I/O request)가 발생하면 kernel이 해당 process를 깨워준다.  

<br>

read_set과 ready_set을 따로 두는 이유  
read_set만을 그대로 Select에서 사용할 경우, Set의 값이 바뀌었을 때, 이전의 정보를 기억할 수가 없다. 따라서 ready_set이라는, read_set의 카피본을 만들어서 사용하는 것이다.  
`Signal에서 prev와 비슷한 것 같다.`  

<br>

#### Issue  
- echo 함수는 이전 포스팅에서 코드를 봤다시피, EOF가 입력될 때까지 수행된다.  
- 따라서, echo 함수 처리가 오래걸리거나, 또는 EOF가 도달하지 않으면(끝나지 않으면), 서버는 계속 기다리게 되는 것이다. `이 서버가 Iterative, 단일 프로세스 서버이기 때문이다.`  
- 즉, Client가 서버에 서비스를 요청해놓고서 Connection 이후 EOF를 보내지 않으면 서버가 마비되는 것이다.  

<br>

#### Modeling Logical Flows as State Machines  
<img src="https://velog.velcdn.com/images/junttang/post/858df0e5-63c5-403b-8065-325f614dbf94/image.png" width="500">
  

<br>

```c
/* Logical Flow를 State Machine으로 바라보기 위한 구조체 */
typedef struct {		
	int maxfd;
    fd_set read_set;			// Active Descriptor로 된 비트 벡터
    fd_set ready_set;			// read_set의 부분집합
    int nready;					// Pending Input이 있는 fd의 개수
    int maxi;
    int clientfd[FD_SETSIZE];   // Active Descriptor로 된 비트 벡터 집합
    rio_t clientrio[FD_SETSIZE];// RIO Package 사용 시의 버퍼
} pool;							// pool이라는 이름의 구조체로 정의

void init_pool(int listenfd, pool *p) {
   	p->maxi = -1;
    for (int i = 0; i < FD_SETSIZE; i++)
    	p->clientfd[i] = -1;	// clientfd를 모두 -1로 초기화
    
    p->maxfd = listenfd;		// maxfd값은 listenfd값으로 설정
    FD_ZERO(&p->read_set);		// Bit Vector도 모두 0으로 초기화
    FD_SET(listenfd, &p->read_set);	// listenfd는 Active로 설정
}

void add_client(int, pool*);
void check_client(pool*);

int main(int argc, char **argv) {
	int listenfd, connfd;
    socklen_t clientlen;
    struct sockaddr_storage clientaddr;
    static pool pool;
    
    if (argc != 2)
    	unix_error("Usage Error!\n");
    
    listenfd = Open_listenfd(argv[1]);		// listen까지의 작업 수행
    init_pool(listenfd, &pool);				// pool 구조체를 초기화
    
    while(1) {
    	pool.ready_set = pool.read_set;		// read_set을 Copy
        pool.nready = Select(pool.maxfd + 1, &pool.ready_set, 
        	NULL, NULL, NULL); 				// Pending Input 존재 fd 개수 반환
        
        if (FD_ISSET(listenfd, &pool.ready_set)) {	// listenfd에 Event 있으면,
        	clientlen = sizeof(struct sockaddr_storage);	// (중요 작업)
            connfd = Accept(listenfd, (SA *)&clientaddr, &clientlen);// Accept!
            add_client(connfd, &pool);		// 생성된 connfd를 pool에 업데이트!
        }									// else는 굳이 하지 않음. 매번 확인하려고
        
        check_client(&pool);		// listenfd 말고, connfd에서 이벤트 발생 시!
    }
}

void add_client(int connfd, pool *p) {
	int i;
	p->nready--;		// Pending Input이 있는 fd 하나를 처리했으므로!
    
    for (i = 0; i < FD_SETSIZE; i++) {		// fdset을 쭉 순회하면서
    	if (p->clientfd[i] < 0) {			// 비어있는 Array 위치에
        	p->clientfd[i] = connfd;		// connfd를 삽입
            Rio_readinitb(&p->clientrio[i], connfd);// 도중에 입력된 데이터 받기
            
            FD_SET(connfd, &p->read_set);	// connfd가 Active 상태로!
            
            if (connfd > p->maxfd)			// File Descriptor Table 상에서
            	p->maxfd = connfd;			// 가장 인덱스가 큰 connfd인 경우, Update!
            if (i > p->maxi)				// connfd Array에서 가장 큰 인덱스
            	p->maxi = i;				// 인 경우에도 Update!
            
            break;
        }
    }
    
    if (i == FD_SETSIZE)
    	app_error("Error in add_client!\n");
}

void check_client(pool *p) {
	int n, connfd;
    char buf[MAXLINE];
    rio_t rio;
    
    // nready가 남아있는 경우, connfd Array를 쫘악 훑는다. 
    for (int i = 0; (i <= p->maxi) && (p->nready > 0); i++) {
    	connfd = p->clientfd[i];
        rio = p->clientrio[i];
        
        // 현재 조회중인 connfd에 Pending Input이 있다면,
        if ((connfd > 0) && (FD_ISSET(connfd, &p->ready_set))) {
        	if ((n = Rio_readlineb(&rio, buf, MAXLINE)) != 0) {		// 읽어들이고
        	    printf("Server received %d bytes on fd %d\n",n, connfd); // 출력
        	    Rio_writen(connfd, buf, n);							// 및 Write
        	}
        	else {								// connfd에서 EOF를 만난 경우,
        		Close(connfd);					// 디스크립터를 닫고
        	    FD_CLR(connfd, &p->read_set);	// fdset에서 Inactive로 만들고
        	    p->clientfd[i] = -1;			// connfd Array에서도 제거!
        	}
        }
    }
}
```

<br>

#### Pros & Cons  
- + 하나의 Logical Control Flow와 Adress Space로 구동할 수 있다.  
- + 디버깅이 쉽다.  
- - 나머지 두 방법론에 비해서 코드가 복잡하다.  
- - 프로그래머가 직접 Fine-Grained Concurrency를 실현해야하기 때문에 프로그래머의 실력이 중요하다.  
- - 단일 process이기 때문에 단일 CPU에서 돌아간다. 즉, Multicore의 장점을 사용할 수 없다.  

<br>

## 6.4 View of a Process  

<br>

#### Hardware View of Execution Flow  
<img src="https://velog.velcdn.com/images/junttang/post/eb83fb5e-5bcc-402d-8a0e-b41d888ef337/image.png" width="400">
  
Process Pa가 생기면, 메인 메모리(DRAM, Volatile Memory)에 해당 프로그램의 *Sequence of Instructions{Code-Data-Stack-Heap}* 가 적재된다.  

<br>

동적인 메모리들이 Stack(Local Variable, Function Return Address)과 Heap(Dynamic Allocation Variable)에 들어간다. 이때 *Stack과 Heap은 서로 마주보면서 나아가는 방향으로 공간을 차지한다.*  

<br>

Process{ Object Code, Machine Code, Instruction Set }  
CPU{  
Register Set{PC/IP(Program Counter, Instruction Pointer), IR(Instruction Register), SP(Stack Pointer), 범용 Register(EAX..)},  
ALU(Arithmetic & Logical Unit),  
CU(Control Unit)  
}  
Bus{ Address Bus, Control Bus, Data Bus } `CPU와 Main Memory 사이 data 교환`  

<br>

CPU가 구동되면, PC를 Increment 해가며 *Instruction Fetch -> Instruction Decode -> Execution -> Write Back*의 과정이 반복적으로 수행된다. CPU의 입장에서 Fetch를 위해 Pa의 Code memory 영역 내에 저장된 명령어를 가져와야 하는 것이다.  

<br>

#### Traditional View  
> Process = process context(program context + kernel context) + code, data, stack`(heap, DRAM에 적재된 process의 영역들)`  
<img src="https://velog.velcdn.com/images/junttang/post/3667c34e-de61-4a97-b8f4-797ccb2025d1/image.png" width="400">
  
- 전부 다 개인적 부분이다. 즉, Overhead가 심하다.  
- Process Context:  
- Data Registers  
- Condition Codes  
- Stack Pointer  
- Program Counter  
- Kernel Context:  
- VM Structures  
- File Descriptor Table  
- brk Pointer  

<br>

#### Alternate View  
> Process = thread(program context + stack) + code, data, kernel context  
<img src="https://velog.velcdn.com/images/junttang/post/8672ae83-bdce-417e-8c73-bbcb79df39c1/image.png" width="400">
  
- thread(개인적 부분) + code, data, kernel context(공유되는 부분)  
- Thread는 Heap과 Code, Data 부분은 그대로 공유하고, Stack만 별도로 가지는 가상의 process라고 생각하면 된다.  

<br>

<img src="https://velog.velcdn.com/images/junttang/post/00fbefd8-3777-4861-a8ec-7465dcbdfa8c/image.png" width="400">
  
> 변화된 것은 없다. 오로지 관점만 바뀐 것이다.  

<br>

#### Thread Details  
<img src="https://velog.velcdn.com/images/junttang/post/1c2aea01-1093-4563-9f4f-9f1d69eeff38/image.png" width="400">
  
- Process 당 여러 개의 Thread, 각 Thread는 별도의 Stack을 가진다. `타 thread에서 접근 가능, 보안에 취약` 또한 고유의 Thread ID인 `TID`를 가진다.  
- 하나의 Process(Thread)에서 Thread를 새로 생성하면, 이를 `Peer Thread`라고 부른다.  
- Hierarchycal 하지 않다.  

<br>

<img src="https://velog.velcdn.com/images/junttang/post/bf8800f4-199e-4a11-bf90-a49695b66276/image.png" width="400">
  
- T1 & T2, T1 & T3가 서로 Concurrent Flow 관계이다. 한편, T2 & T3는 각 Thread의 시작과 끝이 겹치지 않으므로 Sequential Flow 관계이다.  

<br>

<img src="https://velog.velcdn.com/images/junttang/post/8cc4561c-d950-4ef1-a503-8e10715b3725/image.png" width="400">
  
- Single Core이면 겹칠 수 없고, Multiple Core이면 두 개를 겹칠 수 있다. 한편, 세 Thread가 모두 Concurrent flow 관계이다.  

<br>

#### Thread vs Process  
- Similar:  
- 둘 다 고유의 Logical control flow, Execution flow를 가진다.  
- 둘 다 Concurrent flow를 가질 수 있다.  
- 둘 다 Contex switch에 영향을 받는다.  
- Different:  
- Thread는 Local variable을 위한 Stack을 제외하고는 모두 공유한다.  
Code/Data/Kernel Context를 공유한다.  
Process는 모두 별도로 존재한다.  
- Thread 관리는 Process 관리에 비해 Overhead가 상대적으로 덜하다. `약 2배의 차이`  

<br>

## 6.5 Posix Thread (Pthreads) Interface  

<br>

#### Pthreads  
```c
pthread_create()			// 프로세스의 fork같은 개념
pthread_join()				// 프로세스의 wait같은 개념
pthread_self()				// TID를 알아낸다.
pthread_cancel()			// Thread 종료
pthread_exit()				// Thread 종료
exit()						// 모든 Thread 종료
pthread_mutex_init          // Shread variable에 대한 접근 Synchronizing
pthread_mutex_[un]lock      // Shread variable에 대한 접근 Synchronizing
```

<br>

#### Pthread based "hello, world" Program  
```c
/* hello.c - Pthreads "hello, world" program */
#include "csapp.h"
void *thread(void *vargp);

int main()
{
	pthread_t tid;
	Pthread_create(&tid, NULL, thread, NULL);
	Pthread_join(tid, NULL);
	exit(0);
}

void *thread(void *vargp) /* thread routine */
{
	printf("Hello, world!\n");
	return NULL;	
}
```
Pthread_create(Thread ID, Thread attributes(usually NULL),  
 Thread routine, Thread arguments (void *p))  
Pthread_join(Thread ID, Return value (void **p))  
- 위 코드의 과정은 다음과 같다.  
1. Main thread  
2. call Pthread_create()  
3. Peer thread  
4. Pthread_create() returns  
5. call Pthread_join() `Main thread waits for peer thread to terminate`  
6. Peer thread routine  
7. Peer thread terminates  
8. Pthread_join() returns  
9. exit() `Terminates main thread and any peer threads`  

<br>

## 6.6 Thread-Based Server  

<br>

6.2의 process-based server를 복기하자.  
> 기본적으로 Thread-based server는 process-based 방법론과 매우 유사하다.  
> 단지 Process 대신 Thread를 이용하는 것이다.  
- Overhead가 상대적으로 적다.  
- 다만 다른 Thread의 Stack 참조를 막을 수 없어 보안적 측면에서는 취약하다.  

<br>

```c
/* Routine of Peer Thread */
void *thread(void *vargp);								// 후술

/* Thread-based Concurrent Server */
int main(int argc, char **argv) {
	int listenfd, *connfdp;
    pthread_t tid;
	socklen_t clientlen;
	struct sockaddr_storage clientaddr;

	listenfd = Open_listenfd(argv[1]);				// listen까지의 작업 수행
	while (1) {
		clientlen=sizeof(struct sockaddr_storage);	// 늘 말하듯이 중요한 과정
		connfdp = Malloc(sizeof(int)); 
		*connfdp = Accept(listenfd, (SA *) &clientaddr, &clientlen);
		// Accept!
		Pthread_create(&tid, NULL, thread, connfdp); // Peer Thread로 만들자!
	}
}
```
- connfdp라는 포인터 변수를 만든다. `Main Thread의 Stack 공간에 포인터 변수로서 존재` Heap 공간에 있는 Word Size 공간을 가리킨다.  
- Main Thread가 Pthread_create를 하면, Peer Thread에게 connfdp가 가리키는 Heap 공간을 넘긴다.  
- 그래서 따로 Close하지는 않는다.  
- listenfd도 별도의 공간에 있는 것이 아니니까 닫을 필요가 없다.  

<br>

```c
/* Routine of Peer Thread */
void *thread(void *vargp) {
	int connfd = *((int *)vargp); // 넘겨받은 Heap 공간값을 connfd가 가리킨다.
	Pthread_detach(pthread_self());  	// 아래에서 설명할 것! (Reaping 관련)
	Free(vargp); 						// 값을 추출했으므로, 힙공간은 해제하자.
	echo(connfd);						// connfd에 대해서 Service를 제공한다!
	Close(connfd);						// 서비스 끝나면 디스크립터를 닫자!
	return NULL;				// pthread_create 함수에게 NULL을 넘김(관습)
}
```
- OS Kernel이 TID에 해당하는 Thread를 알아서 Reaping해주도록 설정하는 역할  
- 다른 Thread와 상관없이 독립적으로 수행된다.  

<br>

<img src="https://velog.velcdn.com/images/junttang/post/54abfaea-1353-4d0c-ac78-ad3f4cb8f381/image.png" width="500">
  
- 각 Client는 개별 Peer thread에 의해 핸들링된다.  
- 각 Thread는 TID를 제외한 모든 Process(Main Thread) State를 공유한다.  

<br>

#### Issue  
- 반드시 `Pthread_detached(), Pthread_join()`로 Thread Reaping을 해야 한다. `Memory leakage 문제`  
- Unintended Sharing을 주의해야 한다.  
- passing pointer to main thread's stack `Pthread_create(&tid, NULL, thread, (void*) &connfd);`  
- Create를 끝내기 전에 다른 connection이 와서 또 다른 peer thread2를 만들었다면 기존 peer thread1의 accept의 return 값인 connfdp가 10에서 11로 바뀔 수가 있다.  
- 위 문제를 해결하기 위해 Thread-Safe한 함수들을 사용해야 한다.  

<br>

#### Pros & Cons  
- + Thread 끼리 자료구조의 공유가 용이하다. `IPC 같은 것이 불필요`  
- + Process-based에 비해 성능이 우수하다.  
- - 자료구조의 공유가 반대로 매우 위험하게 작용할 수 있다.  
- 어떤 데이터가 공유되는지 알기 어렵다.  
- Corruption이 발생했을 때 알아차리기가 어렵다.  
- Race가 매우 미묘하고, 가끔 발생하기 때문에 알아차리기가 어렵다.  

<br>

# 7. Synchronization: Basics  

<br>

> 변수 x가 Shared라는 것은, 복수의 Thread가 해당 x의 인스턴스를 참조한다는 것을 말한다.  
> 우리는 다음과 같은 질문들을 던질 수 있다.  
- Thread의 Memory Model이란 무엇인가?  
- 변수의 인스턴스는 Memory에서 어떻게 mapped되는가?  
- 이런 인스턴스를 얼마나의 thread가 참조하는가?  

<br>

## 7.1 Thread Memory Model  

<br>

개념과 실제 동작의 괴리에 대해서 알아보자.  

<br>

#### Conceptual Model  
>   

<br>

#### Reality  
>   

<br>

### 7.1.1 Data Sharing via Indirection  

<br>

```c
void *thread(void *vargp);		// Thread Routine
char **ptr; 					// 모든 Thread가 다 같이 접근 가능할 수 있는 전역변수

int main(void) {
	pthread_t tid;
	char *msgs[2] = {				// 이들은 모두 main Thread의 Local Variable들!
		"Hello, I'm foo.\n",		// 즉, main의 Stack에 존재한다.
		"Hello, I'm bar.\n"		// 그말은 즉, Peer Thread들이 접근하지 못해야한다.
	};								// (Conceptual 관점에서 말이다)				

	ptr = msgs;			// 전역변수 ptr이 msgs라는 Local Variable의 주소를 담는다. ★
	for (long i = 0; i < 2; i++)						// i는 0 아니면 1이고,
		Pthread_create(&tid, NULL, thread, (void *)i); 	// i가 Argument로 넘어간다.
	Pthread_exit(NULL);								// 여긴 Thread Reaping하는 상황
}

void *thread(void *vargp) {					// Thread 0번과 1번이 이를 같이 수행한다.
	long myid = (long)vargp;
	static int cnt = 0;		// Local Variable이지만, static이므로 Data부에 들어간다.
							// (cnt의 Lifetime은 thread함수의 시작과 끝이다.)
	printf("[%ld]: %s (cnt=%d)\n", myid, ptr[myid], ++cnt);	// myid는 0 또는 1일 것
							// ptr은 전역변수이고!
	return NULL;
}
// ~> 따라서, i가 0인 Thread는 foo를, 1인 Thread는 bar를 출력할 것이다.
```

<br>

<img src="https://velog.velcdn.com/images/junttang/post/b7ef4e0c-a6e9-4b9e-8c18-eb0f578de17e/image.png" width="400">
  

<br>

#### Variables  
- Global variables:  
Def. 함수 밖에서 정의된 변수  
각 global var에 대해 Virtual Memory 상에서 오직 하나의 Instance만 존재한다.  
- Local variables:  
Def. 함수 내에서 정의된 변수  
각 Thread의 Stack에는 각 Local var의 Instance가 하나씩 존재한다.  
- Local Static variables:  
Def. Static Keyword와 함꼐 선언된 local variable  
각 Static var에 대해, Virtual Memory 상에서 오직 하나의 Instance만 존재한다.  
- 함수의 시작과 끝, 그 사이에서만 유효한데, 그동안은 전역 변수처럼 Data Segment에 저장한다.  

<br>

#### Mapping Variable Instances to Memory  
위 코드에서 각 변수는 다음과 같이 mapping이 된다.  
<img src="https://velog.velcdn.com/images/junttang/post/57338642-c884-4b57-9765-44fde117c4ac/image.png" width="400">
  
> Shared: ptr, cnt, msgs  
> Not Shared: tid, i, myid  
- ptr: Data Segment  
- cnt: Data Segment (Local Variable이지만, static으로 선언되었으므로)  
- myid: 각 Thread의 Stack  
- tid, msgs, i: Main Thread의 Stack  

<br>

## 7.2 Synchronization Problem  

<br>

#### badcnt.c: Improper Synchronization  
```c
void *thread(void *vargp);	// Thread Routine

volatile long cnt = 0;		// Global shared variable : Counter (in Data)
							// volatile 선언은 하단에서 설명한다.
int main(int argc, char **argv) {		// 이 프로그램은 Argument를 받는다.	
	pthread_t tid1, tid2;				// 예를 들어, ./example 10000이란 명령으로
	long niters = atoi(argv[1]);		// 프로그램을 수행했다고 가정하자.

	Pthread_create(&tid1, NULL, thread, &niters);	// Thread를 두 개 띄운다.
	Pthread_create(&tid2, NULL, thread, &niters);	// (T0, T1)
	Pthread_join(tid1, NULL);						// Join한다. (Reaping)
	Pthread_join(tid2, NULL);						// Joinable Threads

	if (cnt != (2 * niters))			// 결과가 20000이 아니면 잘못된 상황이다.
		printf("Shit! cnt is %ld\n", cnt);
	else										// 20000이 되길 기대하는 상황
		printf("Yeah! cnt is %ld\n", cnt);		// 아래의 루틴을 보면 이해 가능
	exit(0);
}

void *thread(void *vargp) { 	// Thread Routine
	long i, niters = *((long *)vargp);	// 넘어온 Iteration Number에 대해,
											// 가정에 따르면 10000이다.
	for (i = 0; i < niters; i++)		// 10000번을 돌면서 Increment!
		cnt++; 
        
	return NULL; 
} 
```

<br>

```
linux> ./badcnt 10000
OK cnt=20000
linux> ./badcnt 10000
BOOM! cnt=13051
linux>
```
어떨 때는 의도한 결과가, 어떨 때는 잘못된 결과가 나오고 있다.  

<br>

#### Problem: Conflict  
> Thread Routine에서 전역변수 cnt를 각 Thread가 함께 접근하고 있는데, 그 cnt를 접근하는 명령이 Atomic하다고 착각한 것이다.  
- Atomic Operation:   

<br>

##### C -> Assembly(ASM): Main Thread Loop  
```c
for (i = 0; i < niters; i++)
	cnt++;
```

<br>

```null
	movq (%rdi), %rcx
	testq %rcx,%rcx
	jle .L2
	movl $`0, %eax
.L3:
	movq cnt(%rip),%rdx		// Load			(여기는 cnt++; 부분)
	addq `$1, %rdx			// Update		(여기는 cnt++; 부분)
	movq %rdx, cnt(%rip)	// Store		(여기는 cnt++; 부분)
	addq $`1, %rax
	cmpq %rcx, %rax
	jne .L3
.L2:
```
ASM 단위로 보면 cnt++;라는 명령어가 실제로는 3개의 기계어 명령으로 구성되어 있다.
-> 즉, `cnt++;`는 Atomic하지 않은 3개의 Instruction의 Sequence인 것이다!

이상적인 상황은 다음과 같다.
<img src="https://velog.velcdn.com/images/junttang/post/fdd8d042-5dbe-441c-a6ee-348eee5d7674/image.png" width="400">

색깔로 표현된 부분: Critical Section
초록색: Thread0, 보라색: Thread1
초록색과 보라색 사이의 흰색: Context Switch

Conflict가 난 상황은 다음과 같다.
<img src="https://velog.velcdn.com/images/junttang/post/39b557da-9c93-4b44-91df-a8177feb9b86/image.png" width="600">

Situation1: 최종적으로 cnt가 2가 아니라 1인 상황이다.
Situation2: 최종적으로 cnt가 2가 아니라 1인 상황이다.

-> 즉, Sequentially Consistent Interleaving 상황에 따라 의도와는 동 떨어진 결과가 나타날 수 있는 것이다.

#### Solve: Volatile
> cnt 전역 변수를 volatile로 선언하면 Version & Cocurrency Issue를 막을 수 있다.
- Volatile: `Guidelines for Writing Safe Handlers를 복기하자.`
Main Memory DRAM의 Object와 Cache에 있는 Object가 서로 동기화 되도록 한다.
즉, Main Memory와 Cache의 Version 차이를 방지하는 것이다.
사실상 Cache가 없는 효과를 내는 것이다.

## 7.3 Progress Graphs

`Not Process Graph!`

#### Progress Graph
> Concurrent Flow 관계의 두 Thread의 자세한 명령 진행 상황을 나타낸다.
- 하나의 State를 나아가는데 여러 갈래의 Path가 있다.
- 각 Axis는 Thread의 순차적 명령 수행 순서를 나타낸다.
- 각 Point는 Possible Execution State를 나타낸다.

-> 요는, Unsafe Region(Conflict 구역)을 파악하고 그를 피해가는 Trajectory(궤적)를 파악하는 것이다.
Critical Section으로 이루어진 정사각형 형태의 구역이 Unsafe Region이다.
<img src="Docs/Pasted image 20240521153620.png" width="300">
 <img src="Docs/Pasted image 20240521153630.png" width="300">


## 7.4 Semaphores

#### Mutual Exclusion
> Safe Trajectory를 보장하려면, Critical Section이 수행될 때 두 Thread가 상호 배타적으로(Mutually Exclusive) 수행되어야 한다.
>> 이를 **(Thread) Sunchronization = Mutual Exclusion**이라 한다.
- Classic Solution:
**Semaphores** (Edsger Dijkstra)
`추가로 Mutex & condition variables(Pthread), Monitors(JAVA) 등도 있다.`

#### Semaphores
> Non-negative Global Integer이자 Synchronization Variable로, P와 V라는 연산에 의해 제어된다.
- P(s): `Variable Test + Variable Setting(Decrement)`
Semaphore s가 0인지 확인해, 0이 아닌 값(양수)이면 Decrement하고 종료한다.
만약 s가 0이면, P연산을 수행한 Thread가 Suspend된다.
- s가 0이라는 것은, 어떤한 다른 Thread가 이미 Semaphore를 Decrement해놓고 점유하고 있다는 의미이다.
- s가 다시 0보다 큰 값이 될 때까지 P연산을 호출한 Thread는 Sleep한다. 깨어날 때는 Signal로 깨어난다.
- Sleep한 Thread가 깨어나는 동작은, Sleep한 Thread가 상시로 s를 계속 체크하는 것이 아니라, 타 Thread의 V연산에 의한 Signal이 도달해서 깨어나는 것이다.
- Test와 Decrement는 Atomic하다.
- V(s): `Variable Setting(Increment)`
s를 점유하고 있는 Thread가 점유를 종료할 때 호출되어 s를 Increment한다.
- s를 양수로 increment하고 sleep한 Thread들을 모두 동시에 재실행한다. 재실행된 Thread들을 다시 경쟁시켜 1개를 추려낸다. `교수님은 FIFO 형태로 Thread를 하나씩 깨운다고 하셨다.`
- Increment는 Atomic하다.
`???: P와 V는 Atomic해 둘이 될 수 없어~`
```c
#include <semaphore.h>
int sem_init(sem_t *s, 0, unsigned int val);} 	/* s = val */
int sem_wait(sem_t *s); 						/* P(s), 기다리기 때문에 wait이라 함 */
int sem_post(sem_t *s); 						/* V(s), 이후 연산이라 post라 함 */

// 우리는 sem_wait과 sem_post를 이용해 아래와 같은 Wrapper Function을 만들 수 있다.
void P(sem_t *s);								// 참조 교재의 csapp.h에서 제공
void V(sem_t *s);								// 내부 구현은 생략
```

### 7.4.1 Improper Synchronization `badcnt.c`

```c
/* Global shared variable */
volatile long cnt = 0; /* Counter */

int main(int argc, char **argv)
{
	long niters;
	pthread_t tid1, tid2;
	
	niters = atoi(argv[1]);
	Pthread_create(&tid1, NULL, thread, &niters);
	Pthread_create(&tid2, NULL, thread, &niters);
	Pthread_join(tid1, NULL); Pthread_join(tid2, NULL);
	
	/* Check result */
	if (cnt != (2 * niters))
		printf("BOOM! cnt=%ld\n", cnt);
	else
		printf("OK cnt=%ld\n", cnt); exit(0);
}

/* Thread routine */
void *thread(void *vargp)
{
	long i, niters = *((long *)vargp);
	
	for (i = 0; i < niters; i++)
		cnt++;
		
	return NULL;	
}
```
`volatile`: 컴파일러가 모드 최적화를 하는 과정에서 예상하지 못하는 side effect를 막기 위해서 사용한다.

### 7.4.2 Proper Synchronization `goodcnt.c`

- mutex를 정의하고 초기화 한다. `cnt`
- mutex에 바로 기록할 수 있게 `cnt`를 volatile로 선언한다.
```c
volatile long cnt = 0; 		// Shared Variable (volatile)
sem_t mutex; 				// Binary Semaphore which protects 'cnt'
Sem_init(&mutex, 0, 1); 	// Initial Value of Semaphore is 1

/* Improper Synchronization badcnt.c  동일한 main 함수 : 생략 */

void *thread(void *vargp) { 			// Thread Routine
	long i, niters = *((long *)vargp);
										
	for (i = 0; i < niters; i++) {
		P(&mutex);
		cnt++;					// Critical Section를 P와 V로 감싼다. ★★★
		V(&mutex);				// Synchronization via 'Mutual Exclusion'
	}
        
	return NULL; 
} 
```
- `badcnt.c`보다 느려진 것을 볼 수 있는데, 이는 `badcnt.c`에서는 순서와 값에 상관없이 마구잡이로 하니까 빨랐던 것이다. 즉, 최적화와 정확도는 trade-of 관계인 것이다.

> `sem_wait(P)`와 `sem_post(V)`로 특정 명령 구간을 둘러싼다고 생각하자.

<img src="https://velog.velcdn.com/images/junttang/post/319b4732-2d2f-4a34-9264-1bd26ceca2ab/image.png" width="400">

- P, V 연산으로 인해 Unsafe Region에 들어가기 위해선 `mutex`가 -1이 되어야 한다. 그러나 `mutex`는 unsigned integer이기 때문에 이가 불가능해지는 것이다.
- 둘러싼 구간을 하나의 Instruction처럼 취급하게 되는 것이다.

# 8. Synchronization: Advanced

## 8.1 Producer-Consumer Problem

<img src="https://velog.velcdn.com/images/junttang/post/8889b258-32e1-4764-a47f-ecd78542a0b8/image.png" width="400">


#### Producer-Comsumer Problem
> 공유되는 요소에 대하여 발생할 수 있는 보편적인 문제를 생각하면 된다.
1. Producer가 empty slot을 기다림, buffer에 item을 넣음, Consumer를 알아챔
2. Consumer가 item을 기다림, buffer에서 item을 가져옴, Producer를 알아챔

그러니까... Synchronize하게 써야하는 공유자원(item) 모델을 어떻게 정형화하여 만들거냐
그에 따른 sub operation은 어떻게 정의할거냐 `linkedlist.insert(), linkedlist.delete()과 같이` 를 생각해야 하는 것 같다.

# 9. Thread-Lebel Parallelism

#### Typical Multicore Processor
<img src="https://velog.velcdn.com/images/junttang/post/62330137-8744-4208-9a79-deaa2e6909d1/image.png" width="400">

> 만약 찾는 data가 Regs에 없다면 L1, L2, L3 cache 순서대로 찾아보고 그래도 없으면(miss) Main memory에서 찾아본다.
- L1, L2는 Private cache로 각 Core에 대해 하나씩 존재하고, 해당 Core에서만 접근할 수 있다. L3는 모든 Core가 접근할 수 있는 Shared cache이다.
- L2: Instruction/Data에 대한 것이 구분되어있지 않고 Unified 되어있다.
- L3: Core 밖에 있으며, Core가 구분되어있지 않고 Unified 되어있다.
- 일반적으로 Main memory는 DRAM으로 Cache memory는 SRAM으로 만들며, Refister Set에 가까울수록 수행 속도가 빠른 특성이 있다.

## 9.1 Thread-Level Parallalism

위의 `Typical Multicore Procesor` 구조를 염두한채로 다음 예제들을 살펴보자.
GPU가 SIMD하게 task를 처리하는 방식을 머릿속에 떠올려 보자.

#### Example1: Parallel Summation
> 0부터 n-1까지의 값을 더하는 프로그램을 만들어보자. `$result = \frac{(n-1)\times n}{2}$`
- 다음과 같이 n개의 수(`$[0, n-1]$`)를 `$t$`로 나누면 각 Partition에는 Flooring(`$\frac{n}{t}$`)만큼의 수들이 들어있을 것이다.
<img src="https://velog.velcdn.com/images/junttang/post/6d3de651-1f9d-4361-9393-b15f0376360a/image.png" width="200">

- 이렇게 만들어진 각 Partition마다 Thread를 배치하여 Thread마다 덧셈을 하도록 한다.

이제 이를 코드로 구현한 것을 순차적으로 보도록 하자.

### 9.1.1 Bad Implementation: Mutex

#### `psum-mutex`
> Thread들이 각자의 부분 합을 Global Variable에 업데이트하되, Semaphore를 적용해 Synchronization을 시키는 방

```c
void *sum_mutex(void *vargp); 		// Thread Routine
long gsum = 0; 						// Global Shared Variable for Summation
long nelems_per_thread; 			// 각 Thread에서 합할 Elements의 개수!
sem_t mutex; 						// gsum을 Protect할 Binary Semaphore

int main(int argc, char **argv) {
	long i, nelems, log_nelems, nthreads, myid[MAXTHREADS];
	pthread_t tid[MAXTHREADS];

	nthreads = atoi(argv[1]);				// t
	log_nelems = atoi(argv[2]);				// n
	nelems = (1L << log_nelems);			// 0부터 2^n까지 더할 것(2^n = N)
	nelems_per_thread = nelems / nthreads;	// 더할 Elem 개수, Flooring(N/t)
	sem_init(&mutex, 0, 1);					// Binary Semaphore

	for (i = 0; i < nthreads; i++) {
		myid[i] = i; 
		Pthread_create(&tid[i], NULL, sum_mutex, &myid[i]); // Thread를 띄움
	}
	for (i = 0; i < nthreads; i++)
		Pthread_join(tid[i], NULL); // Reaping Routine

	if (gsum != (nelems * (nelems-1))/2)
		printf("Error: result=%ld\n", gsum); // Sum이 제대로 이루어졌는지 확인

	exit(0);
}

/* Thread Routine */
void *sum_mutex(void *vargp) {
	long myid = *((long *)vargp); 			// 몇 번 Thread인지 기록
	long start = myid * nelems_per_thread; 	// 이 Thread의 Range의 시작 숫자
	long end = start + nelems_per_thread;	// 이 Thread의 Range의 끝 숫자
	long i;

	for (i = start; i < end; i++) { 		// 쭉 더한다.
		P(&mutex); 
		gsum += i; 							// Critical Section 보호하면서!
		V(&mutex); 
	}
		
	return NULL;
}
```

결과
> Thread가 늘어났는데, 예상과 다르게 오히려 성능이 더 안 좋아졌다.
>> 즉, Mutex를 이용하더라도 (Shared Data Structure를 접근하는)Thread들을 사용하는 Thread Program은 Prarallel Execution의 이득을 전혀 보지 못하는 것이다. 오히려, Thread가 늘어날수로 경쟁이 더 심해져 더 느려질 뿐이다.

### 9.1.2 Good Implementation: Array

#### `psum-array`
> `gsum`이라는 Shared Data Stucture를 두지 말고, 하나의 Array를 두어 각 Array Element가 각 Thread에 1대1 대응하게 만들어, Partial Sum을 Array Element에 두는 방식 `simillar to CUDA`
- Mutex를 사용할 필요 자체가 없다. 즉, Thread들이 Mutex를 잡기 위해 Contending하는 작업 자체를 사라지게 하는 것이다.

```c
#define MAX_NUM		10000000
// ...
long psum[MAX_NUM];
// ...

/* Same Main Routine as previous example */
// ...

/* Thread Routine */
void *sum_array(void *vargp) { 
	long myid = *((long *)vargp);
	long start = myid * nelems_per_thread;
	long end = start + nelems_per_thread;
	long i; 

	for (i = start; i < end; i++) 
		psum[myid] += i; 				// Thread 각자가 자신의 Element에 Sum!

	return NULL; 
}
```

결과
> Parallel Execution의 효과를 보고 있다. Thread가 많아질수록 프로그램의 수행속도가 더 빨라지고 있다. Thread가 하나(단일 CPU)일때도 이전 방식에 비해서 훨씬 효율적임을 알 수 있다.
>> 즉, 아무리 Thread를 이용해 Multicore System 상에서 Parallel Execution을 도모하더라고, Threads가 Shared Data Structure를 다같이 접근하는 구조이면 오히려 프로그램 수행 속도가 느려진다는 것을 알 수 있다.
>>> Mutex를 사용하지 않으면 속도가 빠를 것이다. 하지만
>>> Mutex를 사용하지 않으면 Corruption이 일어날 것이다.

### 9.1.3 Greate Implementation: Array with Local

#### `psum-local`
> Iteration 내에서 Array Element에 접근하지 말고 Thread Routine 내에 Temporary Summation Variable `sum`을 하나 마련하고, 거기다 Partial Sum을 수행한 다음에, 최종적인 Sum 결과만 Array Element에 넣는 방식

아래 코드에서 개선 지점을 찾아보자.
```c
for (i = start; i < end; i++) { 
	psum[myid] += i;
}
```
load 연산의 수를 줄일 수 있다.

```c
/* Thread Routine */
void *sum_local(void *vargp) {
	long myid = *((long *)vargp);
	long start = myid * nelems_per_thread;
	long end = start + nelems_per_thread;
	long i, sum = 0;

	for (i = start; i < end; i++)  
		sum += i; 
        
	psum[myid] = sum;
    
	return NULL;
}
```
CUDA에서 load를 줄이기 위해 지역변수 sum을 사용하는 것과 비슷한 것 같다.

결과
> Good Implementation 보다 약 3배정도 속도 향상이 일어나는 것을 볼 수 있다.

### 9.1.4 Performance Evaluation

앞선 Parallel Summation 예제를 통해 Thread-Level Parallelism의 장점을 확인했다.
이번엔, 이러한 Parallel Program의 성능을 평가하는 방법에 대해 알아보자.

#### Measure Parameters
> `$T_k$`: k개 Core를 이용해 프로그램을 수행한 Running Time
- `$Def. \ Speedup: S_p = T_1/T_p$`
- `$Relative\ Speedup\ S_p$`: `$T_1$`이 single-core를 parallel 버전으로 사용했을 때의 시간
- `$Absolute\ Speedup\ S_p$`: `$T_1$`이 single-core를 sequential 버전으로 사용했을 때의 시간
- 보통 `$Absolute\ Speedup\ S_p$`를 기준으로 판단한다.
- `$Def. \ Efficiency: E_p=S_p/p=T_1/(pT_p), \ p=(0, 100]$`
- Parallelism으로 인해 발생하는 Overhead를 측정할 때 유용하다.

#### Example: `psum-local`
<img src="https://velog.velcdn.com/images/junttang/post/dee3a110-c59d-489c-ae5b-6e7d31f8ec23/image.png" width="500">

- Thread 개수가 늘어나면서 성능이 좋아지지만 효율의 손실도 커진다.
- 그러나 16에서는 성능도 안 좋아지는 것을 확인할 수 있다.
- 따라서 이 예시에서는 Thread가 8개일 때가 가장 Best인 것이다.
<img src="https://velog.velcdn.com/images/junttang/post/29faf8a6-9dff-4d7d-bd6c-e2efcf76890c/image.png" width="300">


## 9.2 Multicore Overheads

#### Memory Consistency
> Thread Program의 Result가 Memory Consistency Model에 의존하는 현상
<img src="https://velog.velcdn.com/images/junttang/post/ff520e8c-0244-454a-8ff0-93e0a1628ba8/image.png" width="200">

- 이때 이 둘을 동시에 수행시키면 Interleaving이 가능하기 때문에 Print 결과가 Non-Deterministic 하리라는 것을 우리는 이미 알고 있다.
- 문제점, Non-Coherent Cache Scenario:
Thread1이 a를 바꾸고 아직 main memory에 write를 못했는데 Thread2가 그걸 읽어와서 print를 해 절대 나와서는 안 되는 결과가 나오는 것이다.
- 해결, Snoopy Cache:
Cache의 각 Block에 State를 나타내는 Tag를 붙인다.
- Invalid: Cannot use value
- Shared: Readable copy
- Exclusive: Writeable copy
처음에 값을 바꿀 때는 Exclusive tag를 붙인다.
`$T_2$`가 a를 read 한다고 할 때, 모종의 이유로 a의 값이 update 되었다는 것을 알게되고 다른 Thread에서 a의 값을 가지고 와서 print, 그리고 나서 해당 a의 tag를 shared로 바꾼다. 누가 어떤 tag로 어떤 값을 가지고 있는지를 서로서로 확인할 수 있도록 하드웨어적으로 구현한 것이다.

> Multicore Processor는 이처럼 Sequential Consistency, Snoopy Cache 등의 개념을 실현시키기 위해 내부적인, HW적인 Overhead가 불가피하다.
>> 그리고 이러한 Overhead는 Multicore Processor 위에서 Thread Program을 돌릴 때 어느 순간 변곡점이 발생하는 원인 중 하나로 꼽힌다.

# 10. Dynamic Memory Allocation: Basic Concepts

## 10.1 Basic Concepts

#### Dynamic Memory Allocation (동적 메모리 할당)
> 동적 할당은 Heap Memory MAnager가 수행한다. 프로그래머는 이 Dynamic Memory Allocator를 이용해 Run-Time에 동적할당을 지시한다.
<img src="https://velog.velcdn.com/images/junttang/post/a92b220c-c422-46fb-bfce-a9c9956cc51a/image.png" width="300">

- Types of Allocator:
- Explicit Allocator: `malloc, free`
우리가 직접 메모리 공간을 할당, free 해야한다.
- Implicit Allocator: `garbage collection in JAVA, ML, Lisp`
우리가 직접 free 하지 않고 알아서 사용되지 않은 메모리를 수거해준다.

#### `malloc`
```c
#include <stdlib.h>
void *malloc(size_t size)
```
- 할당 성공 시:
- 프로그래머가 명시한 Size를 최소로 커버할 수 있는 크기만큼 Memory Block을 할당하고, 그에 대한 Pointer를 반환한다.
- Size == 0이면, NULL을 반환한다.
- 할당 실패 시:
- MULL(0)를 반환하고 errno를 세팅한다.
```c
void free(void *p)
```
- 호출 시:
- p가 가리키고 있는 Memory Block를 다 해제한다.
- free에서 넘겨받는 p는 반드시 이전에 malloc으로 heap 영역 할당이 이루어진 포인터여야 한다.
```c
calloc()
realloc()
sbrk()
```
- 이 외에도 `calloc`, `realloc`, `sbrk` 등이 있다.
- `calloc`: malloc + 할당된 메모리 공간을 0 value로 초기화
- `realloc`: 기존에 allocated인 block의 size를 바꿀 때 사용
- `sbrk`: brk를 올리거나 내려서 heap 공간 조절에 사용

#### Simple Simulation
<img src="https://velog.velcdn.com/images/junttang/post/abe2ab09-7a4c-4dd3-8b7b-a7589ce7c731/image.png" width="500">

> 메모리 할당 시 "가능한 공간 중 가장 맨 앞"에 할당한다.

#### Constraints
> 최초의 Dynamic Memory Allocator를 설계한다고 생각해보자.
> 다음과 같은 제약 사항을 상정할 수 있다.
- Applications:
- free는 malloc이 되어있는 block에 적용해야한다.
- Allocators:
- 

#### Peak Memory Utilization
-  **Def. Aggregate Payload** `$P_k$`
현재 할당된 Payload(실제로 요청한 메모리의 크기)의 총 합. 
-  **Def. Current Heap** `$H_k$`
Heap의 시작 주소부터 brk Pointer가 가리키는 위치까지의 Heap 크기
- **Def. Peak Memory Utilization after `$k+1$` requests** `$U_k$`
`$U_k = (max_{i \leq k} \  P_i) / H_k$

<br>

### 10.1.1 Fragmentation  

<br>

#### Fragmentation  
> Fragmentation즉, 단편화 문제는 Poor Memory Utilization으로부터 비록된다.  
> Internal fragmentation과 External fragmentation 문제가 있다.  
- Internal Fragmentation:  
Payload가 실제 할당 크기(Block Size)보다 작은 상황  
<img src="https://velog.velcdn.com/images/junttang/post/e1990030-9e0d-4b93-9c36-0a849f852ce0/image.png" width="400">
  
- External Fragmentation:  
Aggregate Heap Memory는 충분한데, 개별적인 Single Free Block의 크기들이 충분하지 않은 상황  
<img src="https://velog.velcdn.com/images/junttang/post/b69e999b-306e-4dcb-b4a0-048a079f7517/image.png" width="400">
  

<br>

### 10.1.2 Implementation Issue  

<br>

#### How Much to Free?  

<br>

#### How to keep Track of Frees?  
- Method 1: Implicit List  
- Method 2: Explicit List  
- Method 3: Segregated Free List  
- Method 4: Blocks Sorted by Size  

<br>

## 10.2 Implicit Free Lists  

<br>

`필기 속도가 강의 속도를 따라가지 못한다.....`  

<br>

19p.  
header(?) 낭비를 줄이기 위해 두 개의 정보를 두 공간에 넣던 걸 한 공간에 한꺼번에 넣자.  

<br>

20 p.  
8/0 : 8 byte, 1 공간을 쓰고 있고 0, free 되어있는 상태  
16/1 : 16 byte, 2 공간을 쓰고 있고 1, allocated 되어있는 상태  

<br>

단점: 어쨌든 free list를 찾아가야하는 단점이 있다.  

<br>

21p.  
#### Finding a Free Block  
First fit: 처음부터 찾는다.  
```c
/* First Fit Method (When Word is 4Bytes) */
p = start; 
while ((p < end) && 			// Implicit List의 각 블록을 순회 
	((*p & 1) || 				// LSB Check : Allocated Block은 Pass!
	(*p <= len))) {  			// Block Header Value가 Fit하지 않으면 Pass!
	p = p + (*p & -2); 			// Next Block으로 이동 (Alignment 지키면서)
}
```
- & 연산: 값이 참인지를 보는 것'  
- -2의 2's complement: 11111101 + 1 = 11111110, flag를 제외하고 size를 Get하는 과정  
Next fit:  
- 찾으면 찾은 지점에서부터 찾는것 (history가 있는 것)  
- 일반적으로는 First fit보다 수행 속도가 빠르다.  
- Fragmentation 측면에서 안 좋다는 연구 결과가 있다.  
Best fit:  
- 다익스트라 하게 처음부터 끝까지 다 찾는다.  
<img src="https://velog.velcdn.com/images/junttang/post/4e3117fe-7a8f-43ef-9486-231a8d601880/image.png" width="400">
  

<br>

#### How to Allocate in Free Block  
> Free Block 다음은 Allocation이다. 그러나 단순히 Flag만 바꾼다고 되는 것이 아니다.  
> Free Block이 Allocation Size에 완벽하게 Fit 한게 아닌 이상, 여유분이 남을 것이기 때문에 *Splitting*이 필요하다.  
<img src="https://velog.velcdn.com/images/junttang/post/94e16025-bdf5-4156-b43b-7083614e47a1/image.png" width="400">
  
- Splitting을 수행하기 위해 다음과 같은 코드를 작성할 수 있다.  
```c
/* Implicit List Method - How to allocate in Free Block? */
/* We need to split!! (if needed) */
void addblock(ptr p, int len) {				// p라는 블록에 len만큼을 새로 할당
	int newsize = ((len + 1) >> 1) << 1; 	// 할당하고자 하는 크기를 짝수로 만든다
	int oldsize = *p & -2; 					// 선택된 Free Block의 LSB를 0으로!
    
	*p = newsize | 1; 						// Header의 Size&Status 정보를 수정!
											// newsize와 Allocated State로!!!
	if (newsize < oldsize)					// 만약 여유분이 있다면?
		*(p+newsize) = oldsize - newsize; 	// 여유분의 첫 Word를 New Header로!
} 					// Size만 명시하면 된다. Alignment에 의해 LSB가 자동 0이 되므로
```

<br>

#### Freeing a Block  
> Block을 Free 해주는 것도 단순히 Flag만 바꿔준다고 되는 것이 아니다.  
> Free 된 공간 앞 뒤로 붙어있을 수 있는 Another Free Block과 합쳐주기 위해 *Join(Coalesce)* 해주어야 한다.  
<img src="https://velog.velcdn.com/images/junttang/post/92f6445d-5127-4a16-8c6f-8e22f149b7ec/image.png" width="400">
  
- Coalescing을 수행하기 위해 다음과 같은 코드를 작성할 수 있다. (next block이 free인 경우만 고려했을 시)  
```c
/* Implicit List - How to free Allocated Block? */
void free_block(ptr p) {		// 해제하고자 하는 Block을 가리키는 Pointer
	*p = *p & -2; 				// 우선 Flag부터 Unset한다. (Free로)
	next = p + *p; 				// 다음 Block의 위치를 계산해놓는다.
    							// p에다 *p를 Pointer Arithmetic Addition!
	if ((*next & 1) == 0)		// 만약 다음 Block이 Free Block이면?
		*p = *p + *next; 		// p 주소의 Value에 다음 블록 크기까지 합쳐준다.
}
```

<br>

#### Bidirectional Coalescing  
> 그렇다면 next block과 previous block모두를 Colescing 하기위해선 어떻게 해야할까?  
> 이를 위해 *Boundary Tag*라는 개념이 필요하다.  
- **Boundary Tags**:  
Previous Block으로 이동하고 싶으면, 현재 조회 중인 Block의 Header Word의 바로 앞 Word를 확인하면 Previous Block의 Size를 알 수 있다.  
- 이를 위해선 다음과 같이 Block Header를 변형하면 된다.  
<img src="https://velog.velcdn.com/images/junttang/post/457ea2f6-7c79-4917-a2de-246f6ea1f92d/image.png" width="400">
  
- Extra Space를 필요로 한다는 단점이 있긴 하지만, 필요한 희생이라고 보는 것이다.  
- **Case Analysis**:  
Bidirectional Coalescing을 이해하기 위해 다음과 같은 4가지 Case를 상정한다.  
요는 연결되어있는 흰색과 회색을 합쳐주어야 한다.  
<img src="https://velog.velcdn.com/images/junttang/post/b69c6c9f-8f50-43ec-ad8f-ea0efd28f25b/image.png" width="500">
  
- Case1: 그냥 간단하게 Newly Freed Block만 해제하면 된다.  
- Case2: Newly Freed Block과 Next Block을 합체한다.  
- Case3: Newly Freed Block과 Previous Block을 합체한다.  
- Case4: Newly Freed Block과 Previous & Next Block을 합체한다.  
- **Pros and Cons**  
- +) False Fragmentation 문제를 방지한다.  
- -) Internal Fragmentation이 더 심화된다. 즉, 아주 작은 크기의 데이터를 저장할 때 공간이 낭비되는 경향이 있다.  
- 더 Optimize 할 수 있을까?  
- 우리는 Footer를 오로지 Previous Block이 Free인지 확인할 때만 사용한다.  
- 따라서 이전 블록이 Free가 아니라면, 굳이 Footer를 둘 필요가 없다.  
- Double-Word Alighnment 기준으로 LSB 방향 3개 bit는 Unused라 했다. 그중 LSB는 우리가 Flag로 활용하고 있다.  
- 그렇다면 LSB 다음의 두 개 bit로 무언가 해볼만한 일이 있지 않을까?  

<br>

# 11. Dynamic Memory Allocation: Advanced Concepts  

<br>

## 11.1 Explicit Free Lists  

<br>

<img src="https://velog.velcdn.com/images/junttang/post/0ac1af13-0907-4d01-acb8-408c7c7701de/image.png" width="400">
  

<br>

#### Explicit Free Lists  
> **현재 Block의 Size** 뿐만 아니라 **Next Free Block의 주소값**도 저장하는 방식이다. Doubly Linked List와 비슷한 것 같다.  
- Next Free Block이 어디에도 있을 수 있따고 가정하기 때문에, 다음과 같은 추가 정보들이 필요하다.  
<img src="https://velog.velcdn.com/images/junttang/post/4c2ed9fe-27aa-4abb-be96-d92c5eda39e8/image.png" width="400">
  
- Next: forward pointer  
- Prev: back pointer  
- 그만큼 Payload가 줄어들게 된다.  
- Implicit은 State와 상관없이 모든 Block을 순회하는 반면, Explicit은 Free Block만 순회한다.  
- 따라서 순회 횟수가 훨씬 적고, 수행 속도도 더 우수하다.  

<br>

#### Freeing a Block  
> 1. **LIFO(last-in-first-out) Policy**  
> 2. **Address-Ordered Policy**  
1. LIFO Policy:  
- Queue 논리를 따라 간단히 Free List의 맨 앞에 Newly Freed Block을 삽입한다.  
- +) 간단하고, 수행 속도가 빠르다. `O(1)`  
- -) 연구 결과에 따르면 Fragmentation이 Address-Oredered 비해 심하다고 한다.  
2. Address-Oredered Policy:  
- 항상 Address Order에 맞게 Sorting해서 Newly Freed Block을 삽입한다.  
- +) 연구 결과에 따르면 Fragmentation이 LIFO보다 덜 하다고 한다.  
- -) Serch를 필요로 하므로 수행 속도가 상대적으로 느리다. `O(n)`  

<br>

각 방법 별 추가 Case들은 Reference를 참고하도록 하자.  

<br>

## 11.2 Segregated Free Lists  

<br>

<img src="https://velog.velcdn.com/images/junttang/post/2dcccd64-39e8-459f-bec5-e4a442fddc7b/image.png" width="400">
  

<br>

#### Segregated Free Lists  
> Free Block의 Size에 대해 Free List를 각각 마련하는 방식이다. Histogram과 비슷한 것 같다.  
- Unique한 Size가 아니라, Size의 Range를 하나로 묶기도 한다. 너무 많이 나누면 추가 Overhead가 있을 수 있기 때문이다.  
- Segregated Free List의 가장 큰 장점은 위 방식에서 Best Fit을 찾기 위해 전체 Traversal이 필요했던 것과 다르게 요청한 Size에 대응하는 Size Class만 Serch 하면 된다는 것이다. 즉, Free Block Searching의 Cost가 상대적으로 훨씬 적다.  
- 이 방식을 적용한 Dynamic Memory Allocator를 **Seglist Allocator**라고도 부른다.  

<br>


<br>
